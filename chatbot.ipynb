{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebbc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas. ¡Entorno listo con 6 núcleos disponibles!\n",
      "Metadatos cargados y procesados desde el archivo limpio.\n",
      "Creando columna 'Autoría_norm' desde la columna 'Autoría'...\n",
      "Columna 'Autoría_norm' creada exitosamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Autoría</th>\n",
       "      <th>Autoría_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bravo, David; Ruiz-Tagle, Jaime; Sanhueza, Ric...</td>\n",
       "      <td>bravo, david; ruiz-tagle, jaime; sanhueza, ric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raczynski, Dagmar; Pavez, M. Angélica</td>\n",
       "      <td>raczynski, dagmar; pavez, m. angelica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marshall, Guillermo; Correa, Lorena</td>\n",
       "      <td>marshall, guillermo; correa, lorena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marshall, Guillermo; Correa, Lorena</td>\n",
       "      <td>marshall, guillermo; correa, lorena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fernández, Carolina; Jashes, Jessana</td>\n",
       "      <td>fernandez, carolina; jashes, jessana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Autoría  \\\n",
       "0  Bravo, David; Ruiz-Tagle, Jaime; Sanhueza, Ric...   \n",
       "1              Raczynski, Dagmar; Pavez, M. Angélica   \n",
       "2                Marshall, Guillermo; Correa, Lorena   \n",
       "3                Marshall, Guillermo; Correa, Lorena   \n",
       "4               Fernández, Carolina; Jashes, Jessana   \n",
       "\n",
       "                                        Autoría_norm  \n",
       "0  bravo, david; ruiz-tagle, jaime; sanhueza, ric...  \n",
       "1              raczynski, dagmar; pavez, m. angelica  \n",
       "2                marshall, guillermo; correa, lorena  \n",
       "3                marshall, guillermo; correa, lorena  \n",
       "4               fernandez, carolina; jashes, jessana  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 1: Importaciones, Carga y Preparación SIMPLIFICADA de Datos\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import psutil\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "import gradio as gr\n",
    "from IPython.display import display\n",
    "import unicodedata\n",
    "\n",
    "# LlamaIndex Core y componentes\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, StorageContext, Settings, Document,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. Configurar Entorno ---\n",
    "num_cores = psutil.cpu_count(logical=True)\n",
    "os.environ['OMP_NUM_THREADS'] = str(num_cores)\n",
    "os.environ['MKL_NUM_THREADS'] = str(num_cores)\n",
    "print(f\"Librerías importadas. ¡Entorno listo con {num_cores} núcleos disponibles!\")\n",
    "\n",
    "# --- 2. Funciones de Preparación de Datos ---\n",
    "# ¡ACTUALIZADO! Apuntamos al nuevo archivo pre-procesado.\n",
    "METADATA_FILE = os.path.join(\"data\", \"metadatos_con_autoria.csv\") \n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Convierte texto a minúsculas y elimina acentos.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    ).lower()\n",
    "\n",
    "def load_and_prepare_metadata(filepath):\n",
    "    \"\"\"\n",
    "    Carga los metadatos pre-procesados y solo crea la columna 'Autoría_norm'.\n",
    "    Toda la lógica compleja de unión de autores ya no es necesaria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Una buena práctica es seguir rellenando nulos por si acaso.\n",
    "        df.fillna(\"No especificado\", inplace=True)\n",
    "        print(\"Metadatos cargados y procesados desde el archivo limpio.\")\n",
    "\n",
    "        # --- LÓGICA SIMPLIFICADA ---\n",
    "        # Ahora solo necesitamos crear la versión normalizada de tu columna 'Autoría' ya limpia.\n",
    "        print(\"Creando columna 'Autoría_norm' desde la columna 'Autoría'...\")\n",
    "        \n",
    "        # Se asume que la columna 'Autoría' ya existe y está limpia en el CSV.\n",
    "        if 'Autoría' not in df.columns:\n",
    "            raise ValueError(\"El archivo CSV debe contener una columna llamada 'Autoría'.\")\n",
    "            \n",
    "        df['Autoría_norm'] = df['Autoría'].apply(normalize_text)\n",
    "        \n",
    "        print(\"Columna 'Autoría_norm' creada exitosamente.\")\n",
    "        \n",
    "        # Mostramos las dos columnas relevantes para verificar.\n",
    "        display(df[['Autoría', 'Autoría_norm']].head())\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo de metadatos en la ruta: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al cargar o procesar metadatos: {e}.\")\n",
    "        return None\n",
    "\n",
    "# --- Ejecución ---\n",
    "# Cargar y preparar el DataFrame en un solo paso.\n",
    "df_metadatos = load_and_prepare_metadata(METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc27b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API de OpenRouter cargada.\n",
      "Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\n",
      "Configurando el modelo de embeddings con batch size óptimo de 64 para 6 núcleos.\n",
      "Usando Gemini como modelo LLM.\n",
      "\n",
      "Configuración de modelos completada.\n",
      "Uso de memoria actual: 553.27 MB\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: CONFIGURACIÓN OPTIMIZADA DE MODELOS DE IA\n",
    "\n",
    "# Cargar la clave de API desde el archivo .env\n",
    "load_dotenv()\n",
    "if \"OPENROUTER_API_KEY\" in os.environ:\n",
    "    print(\"Clave de API de OpenRouter cargada.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró la clave de API de OpenRouter.\")\n",
    "\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    print(\"Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró el token de Hugging Face.\")\n",
    "\n",
    "# --- Configuración global de LlamaIndex optimizada para CPU ---\n",
    "optimal_batch_size = max(64, num_cores * 8)\n",
    "print(f\"Configurando el modelo de embeddings con batch size óptimo de {optimal_batch_size} para {num_cores} núcleos.\")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    embed_batch_size=optimal_batch_size,\n",
    "    cache_folder=\"./model_cache\",\n",
    "    normalize=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Selecciona el proveedor de LLM aquí: \"openrouter\" o \"gemini\"\n",
    "LLM_PROVIDER = \"gemini\"\n",
    "\n",
    "if LLM_PROVIDER == \"gemini\":\n",
    "    Settings.llm = GoogleGenAI(\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Usando Gemini como modelo LLM.\")\n",
    "else:\n",
    "    Settings.llm = OpenRouter(\n",
    "        model=\"google/gemma-3n-e4b-it:free\",\n",
    "        #model=\"google/gemma-3n-e2b-it:free\",\n",
    "        #model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        #model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        #model=\"deepseek/deepseek-r1-0528:free\",\n",
    "        #model=\"deepseek/deepseek-chat:free\",\n",
    "        #model=\"google/gemini-2.0-flash-exp:free\",\n",
    "        #model=\"mistralai/mistral-nemo:free\",\n",
    "        #model=\"qwen/qwq-32b:free\",\n",
    "        #model=\"microsoft/mai-ds-r1:free\",\n",
    "        #model=\"meta-llama/llama-4-maverick:free\",\n",
    "        temperature=0.1, \n",
    "    )\n",
    "    print(\"Usando OpenRouter como modelo LLM.\")\n",
    "\n",
    "# Función para monitorear uso de memoria\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Uso de memoria actual: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nConfiguración de modelos completada.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbfb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando un nuevo índice...\n",
      "==================================================\n",
      "INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\n",
      "Este proceso solo se ejecutará una vez.\n",
      "==================================================\n",
      "Convirtiendo metadatos en documentos (contenido vs. metadatos)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88456c743174333a6135b9726b09fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando metadatos:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando índice vectorial con FAISS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac585c776f9401e8adef450b8137d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee552f179b024d68bb2a4a557d2eac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el índice FAISS en './storage_index\\faiss_index.bin'...\n",
      "Guardando los componentes de LlamaIndex en './storage_index'...\n",
      "==================================================\n",
      "¡ÍNDICE CREADO Y GUARDADO EN FORMATO COMPATIBLE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Creación o Carga PERSISTENTE del Índice Vectorial (Optimizada para Hugging Face Spaces)\n",
    "\n",
    "# 1. Definimos las rutas donde se guardarán los componentes del índice\n",
    "PERSIST_DIR = \"./storage_index\"\n",
    "FAISS_INDEX_PATH = os.path.join(PERSIST_DIR, \"faiss_index.bin\")\n",
    "DOCSTORE_PATH = os.path.join(PERSIST_DIR, \"docstore.json\")\n",
    "INDEX_STORE_PATH = os.path.join(PERSIST_DIR, \"index_store.json\")\n",
    "\n",
    "# Función para eliminar una carpeta y su contenido de forma segura\n",
    "def remove_directory_safely(directory_path):\n",
    "    import shutil\n",
    "    try:\n",
    "        if os.path.exists(directory_path):\n",
    "            print(f\"Eliminando directorio '{directory_path}'...\")\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directorio '{directory_path}' eliminado exitosamente.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al eliminar directorio: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    # Verificamos si existe el índice FAISS serializado\n",
    "    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(DOCSTORE_PATH) and os.path.exists(INDEX_STORE_PATH):\n",
    "        print(f\"Cargando componentes del índice desde '{PERSIST_DIR}'...\")\n",
    "        try:\n",
    "            # 1. Cargamos el índice FAISS con su método nativo\n",
    "            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "            print(\"Índice FAISS cargado correctamente.\")\n",
    "            \n",
    "            # 2. Creamos el vector store y el storage context\n",
    "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                vector_store=vector_store,\n",
    "                persist_dir=PERSIST_DIR\n",
    "            )\n",
    "            \n",
    "            # 3. Cargamos el índice completo\n",
    "            metadata_index = load_index_from_storage(storage_context)\n",
    "            print(\"¡Índice completo cargado exitosamente!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar algún componente del índice: {e}\")\n",
    "            if remove_directory_safely(PERSIST_DIR):\n",
    "                print(\"Recreando el índice desde cero...\")\n",
    "            else:\n",
    "                raise Exception(\"No se pudo eliminar el índice corrupto. Elimínalo manualmente.\")\n",
    "    \n",
    "    # Si llegamos aquí, es porque necesitamos crear el índice (la carpeta no existe o se eliminó)\n",
    "    if not os.path.exists(PERSIST_DIR) or not os.path.exists(FAISS_INDEX_PATH):\n",
    "        print(\"Creando un nuevo índice...\")\n",
    "        \n",
    "        if df_metadatos is None:\n",
    "            print(\"No se puede crear el índice porque el DataFrame de metadatos no se cargó. Revisa la Celda 1.\")\n",
    "            metadata_index = None\n",
    "        else:\n",
    "            # --- NUEVO Y CORRECTO BLOQUE PARA PEGAR EN CELDA 3 ---\n",
    "\n",
    "            # --- Lógica de Creación de Índice Optimizada ---\n",
    "            print(\"=\"*50)\n",
    "            print(\"INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\\nEste proceso solo se ejecutará una vez.\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            Settings.node_parser = SentenceSplitter(chunk_size=2048)\n",
    "\n",
    "            def optimize_text_for_embedding(text, max_length=1500):\n",
    "                if not isinstance(text, str): return \"\"\n",
    "                text = text[:max_length]\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                return text.strip()\n",
    "            \n",
    "            # --- Definición de la estrategia de contenido ---\n",
    "            \n",
    "            # Columnas con texto rico para la BÚSQUEDA SEMÁNTICA.\n",
    "            columnas_contenido_semantico = [\n",
    "                'Título del estudio', 'Resumen', 'Palabras clave', 'Objetivo', 'Metodología'\n",
    "            ]\n",
    "            # Metadatos categóricos que queremos que sean \"buscables\".\n",
    "            columnas_metadatos_buscables = [\n",
    "                'Categoría', 'Tipo de documento', 'Idioma', 'Incorpora perspectiva de género', 'Entidad a cargo del estudio'\n",
    "            ]\n",
    "            # Columnas que queremos GUARDAR para MOSTRAR en los resultados finales.\n",
    "            columnas_para_metadatos = list(set(\n",
    "                columnas_contenido_semantico + columnas_metadatos_buscables + \n",
    "                ['Autoría', 'Año de publicación', 'Url']\n",
    "            ))\n",
    "\n",
    "            print(\"Convirtiendo metadatos en documentos (contenido vs. metadatos)...\")\n",
    "            metadata_documents = []\n",
    "            for index, row in tqdm(df_metadatos.iterrows(), total=df_metadatos.shape[0], desc=\"Procesando metadatos\"):\n",
    "                \n",
    "                partes_texto_buscable = []\n",
    "                # Añadir contenido semántico principal\n",
    "                for col in columnas_contenido_semantico:\n",
    "                    valor = row.get(col)\n",
    "                    if pd.notna(valor) and str(valor) not in [\"No especificado\", \"\"]:\n",
    "                        texto_optimizado = optimize_text_for_embedding(str(valor))\n",
    "                        repeticiones = 2 if col in ['Título del estudio', 'Resumen'] else 1\n",
    "                        for _ in range(repeticiones):\n",
    "                           partes_texto_buscable.append(f\"{col.replace('_', ' ')}: {texto_optimizado}\")\n",
    "                \n",
    "                # Añadir metadatos clave como frases para mejorar la búsqueda\n",
    "                for col in columnas_metadatos_buscables:\n",
    "                    valor = row.get(col)\n",
    "                    if pd.notna(valor) and str(valor) not in [\"No especificado\", \"\"]:\n",
    "                        partes_texto_buscable.append(f\"Información de {col.replace('_', ' ')}: {str(valor)}.\")\n",
    "\n",
    "                contenido_buscable = \". \".join(partes_texto_buscable)\n",
    "                \n",
    "                # Preparar el diccionario de METADATOS completo para mostrar\n",
    "                metadatos_para_nodo = {\n",
    "                    col: row.get(col, \"No especificado\") for col in columnas_para_metadatos\n",
    "                }\n",
    "\n",
    "                # Crear el Documento con la separación clara de responsabilidades\n",
    "                doc = Document(\n",
    "                    text=contenido_buscable,\n",
    "                    metadata=metadatos_para_nodo\n",
    "                )\n",
    "                metadata_documents.append(doc)\n",
    "            \n",
    "            print(\"Creando índice vectorial con FAISS...\")\n",
    "            d = len(Settings.embed_model.get_text_embedding(\"test\"))\n",
    "            faiss_index = faiss.IndexFlatL2(d)\n",
    "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            \n",
    "            metadata_index = VectorStoreIndex.from_documents(\n",
    "                metadata_documents,\n",
    "                storage_context=storage_context,\n",
    "                show_progress=True \n",
    "            )\n",
    "            \n",
    "            # --- ¡PASOS CRUCIALES DE GUARDADO! ---\n",
    "            # 1. Creamos el directorio si no existe\n",
    "            os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "            \n",
    "            # 2. Guardamos el índice FAISS usando su método nativo\n",
    "            print(f\"Guardando el índice FAISS en '{FAISS_INDEX_PATH}'...\")\n",
    "            faiss.write_index(faiss_index, FAISS_INDEX_PATH)\n",
    "            \n",
    "            # 3. Guardamos el resto del storage context (docstore, index_store)\n",
    "            print(f\"Guardando los componentes de LlamaIndex en '{PERSIST_DIR}'...\")\n",
    "            metadata_index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            print(\"¡ÍNDICE CREADO Y GUARDADO EN FORMATO COMPATIBLE!\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error no manejado: {e}\")\n",
    "    print(\"Por favor, borra manualmente la carpeta 'storage_index' e intenta nuevamente.\")\n",
    "    metadata_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lanzando la interfaz de CemIA con botones...\n",
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda combinada (Embudo Ancho). Tema: 'asistencia', Autor: 'garcia, burgos'\n",
      "Búsqueda semántica por tema: 'asistencia' con filtro de autor: garcia, burgos, k=200\n",
      "Término de búsqueda final: 'asistencia, asistencia escolar, inasistencia, ausentismo, tasa asistencia,  matricula,  participación,  retención'\n",
      "Búsqueda finalizada. Se encontraron 4 documentos.\n",
      "Iniciando búsqueda combinada (Embudo Ancho). Tema: 'asistencia', Autor: 'garcia; burgos'\n",
      "Búsqueda semántica por tema: 'asistencia' con filtro de autor: garcia; burgos, k=200\n",
      "Término de búsqueda final: 'asistencia, asistencia escolar, inasistencia, ausentismo, tasa asistencia,  matricula,  participación,  presentismo'\n",
      "Búsqueda finalizada. Se encontraron 0 documentos.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Lógica del Chatbot e Interfaz Gradio (VERSIÓN DEFINITIVA Y CORREGIDA)\n",
    "\n",
    "if 'metadata_index' not in locals() or metadata_index is None or 'df_metadatos' not in locals() or df_metadatos is None:\n",
    "    print(\"ERROR: El índice o el DataFrame no han sido creados. Por favor, ejecuta las celdas anteriores correctamente.\")\n",
    "else:\n",
    "    # --- FUNCIONES DE APOYO ---\n",
    "    def normalize_text(text):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn').lower()\n",
    "\n",
    "    def get_initial_chat_state():\n",
    "        return {\"user_name\": None, \"current_mode\": \"waiting_for_name\", \"temp_author\": None,\n",
    "                \"last_results\": [], \"current_page\": 0, \"last_search_type\": None}\n",
    "    \n",
    "    def search_by_topic(message, author_filter=None, k=50):\n",
    "        print(f\"Búsqueda semántica por tema: '{message}' con filtro de autor: {author_filter}, k={k}\")\n",
    "        \n",
    "        # --- PROMPT TEMPLATE REESCRITO PARA MÁXIMA CLARIDAD Y SIN ERRORES ---\n",
    "        prompt_text = f\"\"\"Considerando que la búsqueda es en una base de datos de documentos del Ministerio de Educación de Chile, que van de tematicas desde educación parvularia, presupuestos, educación básica, media, superior, educación de adultos, educación técnica y profesional, asistencia a clases, ayudas estudiantiles, junji, junaeb, etc. reescribe la consulta del usuario en una lista de 5 a 10 términos de búsqueda semánticamente relacionados y concisos, separados por comas. Responde SÓLO con los términos.\n",
    "\n",
    "--- EJEMPLO ---\n",
    "Consulta: 'asistencia'\n",
    "Términos: asistencia, asistencia escolar, inasistencia, ausentismo escolar, tasa de asistencia, ayuda, subvenciones, auxilio\n",
    "--- FIN EJEMPLO ---\n",
    "\n",
    "Consulta: '{message}'\n",
    "Términos:\"\"\"\n",
    "        extraction_template = PromptTemplate(prompt_text)\n",
    "\n",
    "        response = Settings.llm.complete(extraction_template.format(query_str=message))\n",
    "        llm_term = str(response).strip().lower().replace('*', '').replace('-', '')\n",
    "        print(f\"Término de búsqueda final: '{llm_term}'\")\n",
    "        \n",
    "        final_query_for_retriever = f\"{message}. {llm_term}\"\n",
    "        \n",
    "        retriever = metadata_index.as_retriever(similarity_top_k=k)\n",
    "        retrieved_nodes = retriever.retrieve(final_query_for_retriever)\n",
    "        results = []\n",
    "        \n",
    "        if retrieved_nodes:\n",
    "            boost_terms = {term.strip() for term in normalize_text(message).split()}\n",
    "            processed_results = []\n",
    "            for node_with_score in retrieved_nodes:\n",
    "                meta = node_with_score.node.metadata\n",
    "                texto_relevante_norm = normalize_text(f\"{meta.get('Título del estudio', '')} {meta.get('Resumen', '')} {meta.get('Palabras clave', '')}\")\n",
    "                bonus = sum(0.1 for term in boost_terms if term in texto_relevante_norm)\n",
    "                processed_results.append({'node_with_score': node_with_score, 'boosted_score': node_with_score.score + bonus})\n",
    "\n",
    "            processed_results.sort(key=lambda x: x['boosted_score'], reverse=True)\n",
    "\n",
    "            normalized_author_filter_parts = normalize_text(author_filter).split() if author_filter else []\n",
    "            all_search_terms = {term.strip() for term in final_query_for_retriever.replace('.',',').split(',') if term.strip()}\n",
    "\n",
    "            for item in processed_results:\n",
    "                node_with_score = item['node_with_score']\n",
    "                meta = node_with_score.node.metadata\n",
    "                \n",
    "                if normalized_author_filter_parts:\n",
    "                    autoria_en_metadata = meta.get('Autoría', '')\n",
    "                    autoria_normalizada = normalize_text(autoria_en_metadata)\n",
    "                    if not all(part in autoria_normalizada for part in normalized_author_filter_parts):\n",
    "                        continue\n",
    "\n",
    "                texto_relevante_norm_pistas = normalize_text(f\"{meta.get('Título del estudio', '')} {meta.get('Resumen', '')} {meta.get('Palabras clave', '')}\")\n",
    "                motivos_encontrados = {term for term in all_search_terms if term in texto_relevante_norm_pistas}\n",
    "                motivo_final = \"\"\n",
    "                if motivos_encontrados:\n",
    "                    motivo_final = \", \".join(sorted(list(motivos_encontrados)))\n",
    "                else:\n",
    "                    palabras_clave_doc = meta.get('Palabras clave', '')\n",
    "                    if palabras_clave_doc and palabras_clave_doc != \"No especificado\":\n",
    "                        motivo_final = f\"Conceptos del documento: {palabras_clave_doc}\"\n",
    "\n",
    "                results.append({\n",
    "                    'Título del estudio': meta.get('Título del estudio', 'N/A'), \n",
    "                    'Autor(es)': meta.get('Autoría', 'Sin información'), \n",
    "                    'Año de publicación': meta.get('Año de publicación', 'N/A'), \n",
    "                    'Url': meta.get('Url', 'No encontrado'),\n",
    "                    '__is_semantic': True, \n",
    "                    'score': node_with_score.score,\n",
    "                    'pistas_relevancia': motivo_final\n",
    "                })\n",
    "\n",
    "        relevant_results = [res for res in results if res[\"score\"] >= 0.40]\n",
    "        return {\"results\": relevant_results, \"original_query\": message, \"expanded_query\": llm_term}\n",
    "    \n",
    "    def search_by_author(authors_query):\n",
    "        print(f\"Búsqueda por autor/es: '{authors_query}'.\")\n",
    "        author_groups = [group.strip() for group in authors_query.split(',') if group.strip()]\n",
    "        \n",
    "        def df_to_results_list(df):\n",
    "            records = []\n",
    "            for _, row in df.iterrows():\n",
    "                records.append({\n",
    "                    'Título del estudio': row.get('Título del estudio', 'N/A'),\n",
    "                    'Autor(es)': row.get('Autoría', 'Sin información'),\n",
    "                    'Año de publicación': row.get('Año de publicación', 'N/A'),\n",
    "                    'Url': row.get('Url', 'No encontrado'),\n",
    "                    'score': 0, 'pistas_relevancia': '', '__is_semantic': False \n",
    "                })\n",
    "            return records\n",
    "\n",
    "        and_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "        for group in author_groups:\n",
    "            normalized_group_parts = normalize_text(group).split()\n",
    "            for part in normalized_group_parts:\n",
    "                and_mask &= df_metadatos['Autoría_norm'].str.contains(part, na=False)\n",
    "        \n",
    "        and_hits_df = df_metadatos[and_mask].copy()\n",
    "\n",
    "        if not and_hits_df.empty:\n",
    "            and_hits_df['Año de publicación num'] = pd.to_numeric(and_hits_df['Año de publicación'], errors='coerce').fillna(0)\n",
    "            sorted_df = and_hits_df.sort_values(by='Año de publicación num', ascending=False)\n",
    "            results_list = df_to_results_list(sorted_df)\n",
    "            header = f\"He encontrado {len(results_list)} documento(s) que incluyen a '{authors_query}'.\"\n",
    "            return {\"results\": results_list, \"header\": header}\n",
    "\n",
    "        if len(author_groups) > 1:\n",
    "            response_parts = [f\"No he encontrado documentos que incluyan a todos estos autores juntos. A continuación, los resultados para cada uno por separado:\\n\"]\n",
    "            any_found = False\n",
    "            for group in author_groups:\n",
    "                group_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "                normalized_group_parts = normalize_text(group).split()\n",
    "                for part in normalized_group_parts:\n",
    "                    group_mask &= df_metadatos['Autoría_norm'].str.contains(part, na=False)\n",
    "                individual_hits_df = df_metadatos[group_mask].copy()\n",
    "                if not individual_hits_df.empty:\n",
    "                    any_found = True\n",
    "                    total_individual = len(individual_hits_df)\n",
    "                    limit_text = f\"(mostrando los 5 más recientes de {total_individual})\" if total_individual > 5 else \"\"\n",
    "                    response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}' {limit_text}:\\n\")\n",
    "                    individual_hits_df['Año de publicación num'] = pd.to_numeric(individual_hits_df['Año de publicación'], errors='coerce').fillna(0)\n",
    "                    sorted_individual_df = individual_hits_df.sort_values(by='Año de publicación num', ascending=False).head(5)\n",
    "                    for i, (_, row) in enumerate(sorted_individual_df.iterrows(), 1):\n",
    "                        response_parts.append(f\"{i}. **Título:** {row.get('Título del estudio', 'N/A')}\\n   - **Autor(es):** {row.get('Autoría', 'Sin información')}\\n   - **Año:** {row.get('Año de publicación', 'N/A')}\\n   - **URL:** {row.get('Url', 'No disponible')}\\n\")\n",
    "                else:\n",
    "                    response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}':\\n\\nNo se encontraron documentos.\")\n",
    "            if any_found:\n",
    "                return {\"results\": [], \"header\": \"\\n\".join(response_parts), \"is_special_format\": True}\n",
    "        return {\"results\": [], \"header\": f\"No he encontrado documentos de '{authors_query}'. Revisa la ortografía.\"}\n",
    "    \n",
    "    def format_results_page(state):\n",
    "        total_found = len(state[\"last_results\"])\n",
    "        if total_found == 0:\n",
    "            return state.get(\"custom_header\") or \"No se encontraron resultados que coincidan con tu búsqueda.\"\n",
    "\n",
    "        MAX_TO_SHOW = 5\n",
    "        start_index = (state[\"current_page\"] - 1) * MAX_TO_SHOW\n",
    "        nodes_to_show = state[\"last_results\"][start_index : start_index + MAX_TO_SHOW]\n",
    "\n",
    "        if not nodes_to_show:\n",
    "            return \"No hay más documentos que mostrar. Ya estás en la última página.\"\n",
    "\n",
    "        header_parts = []\n",
    "        if state.get(\"custom_header\"):\n",
    "            header_parts.append(state.get(\"custom_header\"))\n",
    "        elif state.get(\"last_search_type\") == \"topic\" and state.get(\"expanded_query\"):\n",
    "            header_parts.append(f\"Para tu búsqueda sobre **'{state['original_query']}'**, he consultado los términos: *`{state['expanded_query']}`*.\")\n",
    "        \n",
    "        if total_found > 1:\n",
    "            page_info = f\"(página {state['current_page']} de {-(-total_found // MAX_TO_SHOW)}, mostrando {len(nodes_to_show)} de {total_found} en total)\"\n",
    "            header_parts.append(page_info)\n",
    "        \n",
    "        header = \" \".join(header_parts) + \":\\n\\n\"\n",
    "\n",
    "        lista_formateada = []\n",
    "        for i, res in enumerate(nodes_to_show, start=start_index + 1):\n",
    "            titulo = res.get('Título del estudio', 'N/A')\n",
    "            autores = res.get('Autor(es)', 'N/A')\n",
    "            ano = str(res.get('Año de publicación', 'N/A'))\n",
    "            url = res.get('Url', 'No encontrado')\n",
    "            \n",
    "            relevance_str = \"\"\n",
    "            if res.get('__is_semantic'):\n",
    "                score = res.get('score', 0)\n",
    "                if score > 0:\n",
    "                    relevance_label = \"Muy Alta\" if score >= 0.75 else \"Alta\" if score >= 0.60 else \"Media\"\n",
    "                    relevance_str = f\"**(Relevancia: {relevance_label} [{score:.0%}])**\"\n",
    "            \n",
    "            pistas_str = \"\"\n",
    "            if res.get('pistas_relevancia'):\n",
    "                pistas_str = f\"\\n   - *Pistas de Relevancia: {res.get('pistas_relevancia')}*\"\n",
    "            \n",
    "            lista_formateada.append(f\"{i}. **Título:** {titulo} {relevance_str}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}{pistas_str}\")\n",
    "\n",
    "        footer = f\"\\n\\n--- \\n*Para ver más, escribe **siguiente** o `página {state['current_page'] + 1}`.*\" if (start_index + MAX_TO_SHOW) < total_found else \"\"\n",
    "        return header + \"\\n\\n\".join(lista_formateada) + footer\n",
    "    \n",
    "    def extract_name_from_greeting(message):\n",
    "        patterns = [r\"(?:soy|me llamo|mi nombre es|ll[aá]mame|dime)\\s+(.+)\", r\"^(?:hola,? soy|hola,? mi nombre es)\\s+(.+)\"]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, message, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip().title()\n",
    "        return message.strip().title()\n",
    "\n",
    "    # --- INTERFAZ GRADIO ---\n",
    "    with gr.Blocks(theme=\"default\") as demo:\n",
    "        chat_state = gr.State(get_initial_chat_state())\n",
    "        gr.Markdown(\"# CemIA - Asistente de IA del Centro de Estudios del MINEDUC\")\n",
    "        chatbot = gr.Chatbot(\n",
    "            value=[{\"role\": \"assistant\", \"content\": \"¡Hola! Soy CemIA, asistente de IA del Centro de Estudios del MINEDUC. ¿Cómo quieres que te llame?\"}],\n",
    "            type=\"messages\", \n",
    "            label=\"CemIA\", \n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        with gr.Row(visible=False) as button_row:\n",
    "            # CORRECCIÓN: Se separaron los botones para mayor claridad.\n",
    "            btn_author = gr.Button(\"Búsqueda por autoría\")\n",
    "            btn_topic = gr.Button(\"Búsqueda por temática\")\n",
    "            btn_combo = gr.Button(\"Búsqueda por autoría y temática\")\n",
    "            btn_bye = gr.Button(\"Adiós\")\n",
    "            \n",
    "        user_input = gr.Textbox(placeholder=\"Escribe tu nombre para comenzar...\", label=\"Tu respuesta\", autofocus=True)\n",
    "\n",
    "        def make_buttons_interactive(interactive=True):\n",
    "            return gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive)\n",
    "\n",
    "        def handle_user_input(message, history, current_state):\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            mode = current_state[\"current_mode\"]\n",
    "            \n",
    "            message_lower = message.strip().lower()\n",
    "            if message_lower in [\"siguiente\", \"más\", \"mas\"] or re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower):\n",
    "                if current_state[\"last_results\"]:\n",
    "                    page_match = re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower)\n",
    "                    current_state[\"current_page\"] = int(page_match.group(1)) if page_match else current_state[\"current_page\"] + 1\n",
    "                    history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
    "                else:\n",
    "                    history.append({\"role\": \"assistant\", \"content\": \"No hay resultados anteriores para paginar. Por favor, realiza una nueva búsqueda.\"})\n",
    "                return history, current_state, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "\n",
    "            button_row_update, user_input_update = gr.update(), gr.update(value=\"\", interactive=False, placeholder=\"Elige una opción de los botones\")\n",
    "            btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update = make_buttons_interactive(False)\n",
    "            response_text = \"\"\n",
    "\n",
    "            if mode == \"waiting_for_name\":\n",
    "                user_name = extract_name_from_greeting(message)\n",
    "                current_state[\"user_name\"], current_state[\"current_mode\"] = user_name, \"waiting_for_choice\"\n",
    "                response_text = f\"¡Qué bueno tenerte aquí, {user_name}! Por favor, elige una opción.\"\n",
    "                button_row_update, (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = gr.update(visible=True), make_buttons_interactive(True)\n",
    "            \n",
    "            elif mode == \"waiting_for_author\":\n",
    "                search_data = search_by_author(message)\n",
    "                if search_data.get(\"is_special_format\"):\n",
    "                    response_text = search_data[\"header\"]\n",
    "                    current_state[\"last_results\"] = []\n",
    "                else:\n",
    "                    current_state.update({\"last_results\": search_data[\"results\"], \"current_page\": 1, \"custom_header\": search_data[\"header\"], \"last_search_type\": \"author\"})\n",
    "                    response_text = format_results_page(current_state)\n",
    "                current_state[\"current_mode\"] = \"waiting_for_choice\"\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "            \n",
    "            elif mode == \"waiting_for_topic\":\n",
    "                search_data = search_by_topic(message)\n",
    "                current_state.update({\n",
    "                    \"last_results\": search_data[\"results\"], \n",
    "                    \"original_query\": search_data[\"original_query\"],\n",
    "                    \"expanded_query\": search_data[\"expanded_query\"],\n",
    "                    \"current_page\": 1, \n",
    "                    \"last_search_type\": \"topic\", \n",
    "                    \"current_mode\": \"waiting_for_choice\",\n",
    "                    \"custom_header\": None\n",
    "                })\n",
    "                response_text = format_results_page(current_state)\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "            \n",
    "            elif mode == \"waiting_for_combo_input\":\n",
    "                if '-' not in message:\n",
    "                    response_text = \"Formato incorrecto. Por favor, usa el formato: `temática - autor`. Ejemplo: `asistencia - burgos`\"\n",
    "                    (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "                    return history, current_state, button_row_update, btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update, gr.update(value=\"\", interactive=True, placeholder=\"Inténtalo de nuevo...\")\n",
    "\n",
    "                parts = message.split('-', 1)\n",
    "                topic = parts[0].strip()\n",
    "                author = parts[1].strip()\n",
    "                \n",
    "                print(f\"Iniciando búsqueda combinada (Embudo Ancho). Tema: '{topic}', Autor: '{author}'\")\n",
    "\n",
    "                search_data = search_by_topic(topic, author_filter=author, k=200)\n",
    "                final_results = search_data.get(\"results\", [])\n",
    "                print(f\"Búsqueda finalizada. Se encontraron {len(final_results)} documentos.\")\n",
    "\n",
    "                expanded_query = search_data.get(\"expanded_query\", topic)\n",
    "                custom_header = f\"Para tu búsqueda sobre **'{topic}'** con el autor **'{author}'**, he encontrado {len(final_results)} coincidencias.\"\n",
    "                \n",
    "                current_state.update({\n",
    "                    \"last_results\": final_results,\n",
    "                    \"original_query\": topic,\n",
    "                    \"expanded_query\": expanded_query,\n",
    "                    \"current_page\": 1,\n",
    "                    \"custom_header\": custom_header,\n",
    "                    \"last_search_type\": \"topic\",\n",
    "                    \"current_mode\": \"waiting_for_choice\",\n",
    "                    \"temp_author\": None\n",
    "                })\n",
    "                \n",
    "                response_text = format_results_page(current_state)\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "\n",
    "            else:\n",
    "                response_text = \"Por favor, utiliza uno de los botones para continuar.\"\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "\n",
    "            history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "            return history, current_state, button_row_update, btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update, user_input_update\n",
    "\n",
    "        def handle_choice(history, current_state, choice):\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Has elegido: **{choice}**\"})\n",
    "            mode, prompt_message = \"\", \"\"\n",
    "            \n",
    "            if choice == \"Búsqueda por autoría\": \n",
    "                mode, prompt_message = \"waiting_for_author\", \"Por favor, escribe el nombre del autor/a o autores/as (separados por comas).\"\n",
    "            elif choice == \"Búsqueda por temática\": \n",
    "                mode, prompt_message = \"waiting_for_topic\", \"Por favor, escribe la temática que te interesa.\"\n",
    "            elif choice == \"Búsqueda por autoría y temática\": \n",
    "                mode, prompt_message = \"waiting_for_combo_input\", \"Por favor, dime la temática y el autor(es) que buscas, separados por un guion.\\n**Ejemplo:** `temática - autor(es)\"\n",
    "\n",
    "            current_state[\"current_mode\"] = mode\n",
    "            history.append({\"role\": \"assistant\", \"content\": prompt_message})\n",
    "            b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            return history, current_state, b1, b2, b3, b4, gr.update(value=\"\", interactive=True, placeholder=\"Escribe aquí...\")\n",
    "\n",
    "        def handle_bye(current_state):\n",
    "            if current_state[\"user_name\"]:\n",
    "                gr.Info(f\"Adiós {current_state['user_name']}, ¡recuerda que siempre estoy disponible para asistirte!\")\n",
    "                current_state[\"current_mode\"] = \"ended\"\n",
    "            \n",
    "            b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            ui_input = gr.update(value=\"\", interactive=False, placeholder=\"Conversación finalizada. Recarga la página para empezar de nuevo.\")\n",
    "            return current_state, b1, b2, b3, b4, ui_input\n",
    "\n",
    "        user_input.submit(handle_user_input, [user_input, chatbot, chat_state], [chatbot, chat_state, button_row, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_author.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_topic.click(lambda h, s: handle_choice(h, s, \"Búsqueda por temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_combo.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría y temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_bye.click(fn=handle_bye, inputs=[chat_state], outputs=[chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "\n",
    "    print(\"\\nLanzando la interfaz de CemIA con botones...\")\n",
    "    demo.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
