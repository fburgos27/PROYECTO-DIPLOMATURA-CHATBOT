{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebbc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas. ¡Entorno listo con 6 núcleos disponibles!\n",
      "Metadatos cargados y procesados.\n",
      "Creando columnas normalizadas para la búsqueda de autores...\n",
      "Columnas normalizadas creadas exitosamente.\n",
      "Creando columna de 'Autoría_norm' combinada...\n",
      "Columnas de autoría creadas exitosamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Investigador u organismo principal</th>\n",
       "      <th>Equipo de investigación</th>\n",
       "      <th>Autoría</th>\n",
       "      <th>Autoría_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bravo, David</td>\n",
       "      <td>Ruiz-Tagle, Jaime; Sanhueza, Ricardo</td>\n",
       "      <td>Bravo, David, Ruiz-Tagle, Jaime; Sanhueza, Ric...</td>\n",
       "      <td>bravo, david, jaime, ricardo, ruiz-tagle, sanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raczynski, Dagmar</td>\n",
       "      <td>Pavez, M. Angélica</td>\n",
       "      <td>Raczynski, Dagmar, Pavez, M. Angélica</td>\n",
       "      <td>dagmar, m. angelica, pavez, raczynski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marshall, Guillermo</td>\n",
       "      <td>Correa, Lorena</td>\n",
       "      <td>Marshall, Guillermo, Correa, Lorena</td>\n",
       "      <td>correa, guillermo, lorena, marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marshall, Guillermo</td>\n",
       "      <td>Correa, Lorena</td>\n",
       "      <td>Marshall, Guillermo, Correa, Lorena</td>\n",
       "      <td>correa, guillermo, lorena, marshall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fernández, Carolina</td>\n",
       "      <td>Jashes, Jessana</td>\n",
       "      <td>Fernández, Carolina, Jashes, Jessana</td>\n",
       "      <td>carolina, fernandez, jashes, jessana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Investigador u organismo principal               Equipo de investigación  \\\n",
       "0                       Bravo, David  Ruiz-Tagle, Jaime; Sanhueza, Ricardo   \n",
       "1                  Raczynski, Dagmar                    Pavez, M. Angélica   \n",
       "2                Marshall, Guillermo                        Correa, Lorena   \n",
       "3                Marshall, Guillermo                        Correa, Lorena   \n",
       "4                Fernández, Carolina                       Jashes, Jessana   \n",
       "\n",
       "                                             Autoría  \\\n",
       "0  Bravo, David, Ruiz-Tagle, Jaime; Sanhueza, Ric...   \n",
       "1              Raczynski, Dagmar, Pavez, M. Angélica   \n",
       "2                Marshall, Guillermo, Correa, Lorena   \n",
       "3                Marshall, Guillermo, Correa, Lorena   \n",
       "4               Fernández, Carolina, Jashes, Jessana   \n",
       "\n",
       "                                        Autoría_norm  \n",
       "0  bravo, david, jaime, ricardo, ruiz-tagle, sanh...  \n",
       "1              dagmar, m. angelica, pavez, raczynski  \n",
       "2                correa, guillermo, lorena, marshall  \n",
       "3                correa, guillermo, lorena, marshall  \n",
       "4               carolina, fernandez, jashes, jessana  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 1: Importaciones, Carga y Preparación Completa de Datos\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import psutil\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "import gradio as gr\n",
    "from IPython.display import display\n",
    "import unicodedata\n",
    "\n",
    "# LlamaIndex Core y componentes\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, StorageContext, Settings, Document,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. Configurar Entorno ---\n",
    "num_cores = psutil.cpu_count(logical=True)\n",
    "os.environ['OMP_NUM_THREADS'] = str(num_cores)\n",
    "os.environ['MKL_NUM_THREADS'] = str(num_cores)\n",
    "print(f\"Librerías importadas. ¡Entorno listo con {num_cores} núcleos disponibles!\")\n",
    "\n",
    "# --- 2. Funciones de Preparación de Datos ---\n",
    "METADATA_FILE = os.path.join(\"data\", \"metadatos_chatbot_final.csv\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Convierte texto a minúsculas y elimina acentos.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    ).lower()\n",
    "\n",
    "def load_and_prepare_metadata(filepath):\n",
    "    \"\"\"\n",
    "    Carga los metadatos desde un CSV y realiza toda la preparación necesaria:\n",
    "    - Rellena valores nulos.\n",
    "    - Crea columnas normalizadas para la búsqueda.\n",
    "    - Crea una columna 'Autoría' unificada y limpia a partir de las normalizadas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.fillna(\"No especificado\", inplace=True)\n",
    "        print(\"Metadatos cargados y procesados.\")\n",
    "\n",
    "        # --- Creación de columnas normalizadas para búsqueda eficiente ---\n",
    "        print(\"Creando columnas normalizadas para la búsqueda de autores...\")\n",
    "        df['principal_norm'] = df['Investigador u organismo principal'].apply(normalize_text)\n",
    "        df['equipo_norm'] = df['Equipo de investigación'].apply(normalize_text)\n",
    "        print(\"Columnas normalizadas creadas exitosamente.\")\n",
    "\n",
    "        # --- Creación de la columna de autoría combinada y normalizada ---\n",
    "        print(\"Creando columna de 'Autoría_norm' combinada...\")\n",
    "        \n",
    "        def limpiar_y_unir_autores_normalizados(row):\n",
    "            # --- CAMBIO CLAVE: Usamos las columnas ya normalizadas ---\n",
    "            autores_raw_norm = f\"{row['principal_norm']};{row['equipo_norm']}\"\n",
    "            partes = [a.strip() for a in re.split(r'[;,]', autores_raw_norm)]\n",
    "            # Eliminar vacíos y los valores por defecto ya normalizados\n",
    "            partes_limpias = [p for p in partes if p and p != \"no especificado\" and p != \"sin informacion\"]\n",
    "            \n",
    "            # Quitar duplicados (ya no es necesario, pero es una buena práctica)\n",
    "            autores_unicos = sorted(list(set(partes_limpias)))\n",
    "            \n",
    "            return \", \".join(autores_unicos) if autores_unicos else \"sin informacion\"\n",
    "        \n",
    "        # Creamos una columna de autoría ya normalizada para las búsquedas\n",
    "        df['Autoría_norm'] = df.apply(limpiar_y_unir_autores_normalizados, axis=1)\n",
    "\n",
    "        # También creamos una columna 'Autoría' legible para mostrar al usuario\n",
    "        df['Autoría'] = df.apply(\n",
    "            lambda row: \", \".join(filter(None, [\n",
    "                row['Investigador u organismo principal'] if row['Investigador u organismo principal'] != 'No especificado' else None,\n",
    "                row['Equipo de investigación'] if row['Equipo de investigación'] != 'No especificado' else None\n",
    "            ])) or \"Sin información\",\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        print(\"Columnas de autoría creadas exitosamente.\")\n",
    "        \n",
    "        display(df[['Investigador u organismo principal', 'Equipo de investigación', 'Autoría', 'Autoría_norm']].head())\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo de metadatos en la ruta: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al cargar metadatos: {e}.\")\n",
    "        return None\n",
    "\n",
    "# --- Ejecución ---\n",
    "# Cargar y preparar el DataFrame en un solo paso.\n",
    "df_metadatos = load_and_prepare_metadata(METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc27b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API de OpenRouter cargada.\n",
      "Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\n",
      "Configurando el modelo de embeddings con batch size óptimo de 64 para 6 núcleos.\n",
      "Usando Gemini como modelo LLM.\n",
      "\n",
      "Configuración de modelos completada.\n",
      "Uso de memoria actual: 553.43 MB\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: CONFIGURACIÓN OPTIMIZADA DE MODELOS DE IA\n",
    "\n",
    "# Cargar la clave de API desde el archivo .env\n",
    "load_dotenv()\n",
    "if \"OPENROUTER_API_KEY\" in os.environ:\n",
    "    print(\"Clave de API de OpenRouter cargada.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró la clave de API de OpenRouter.\")\n",
    "\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    print(\"Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró el token de Hugging Face.\")\n",
    "\n",
    "# --- Configuración global de LlamaIndex optimizada para CPU ---\n",
    "optimal_batch_size = max(64, num_cores * 8)\n",
    "print(f\"Configurando el modelo de embeddings con batch size óptimo de {optimal_batch_size} para {num_cores} núcleos.\")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    embed_batch_size=optimal_batch_size,\n",
    "    cache_folder=\"./model_cache\",\n",
    "    normalize=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Selecciona el proveedor de LLM aquí: \"openrouter\" o \"gemini\"\n",
    "LLM_PROVIDER = \"gemini\"\n",
    "\n",
    "if LLM_PROVIDER == \"gemini\":\n",
    "    Settings.llm = GoogleGenAI(\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Usando Gemini como modelo LLM.\")\n",
    "else:\n",
    "    Settings.llm = OpenRouter(\n",
    "        model=\"google/gemma-3n-e4b-it:free\",\n",
    "        #model=\"google/gemma-3n-e2b-it:free\",\n",
    "        #model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        #model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        #model=\"deepseek/deepseek-r1-0528:free\",\n",
    "        #model=\"deepseek/deepseek-chat:free\",\n",
    "        #model=\"google/gemini-2.0-flash-exp:free\",\n",
    "        #model=\"mistralai/mistral-nemo:free\",\n",
    "        #model=\"qwen/qwq-32b:free\",\n",
    "        #model=\"microsoft/mai-ds-r1:free\",\n",
    "        #model=\"meta-llama/llama-4-maverick:free\",\n",
    "        temperature=0.1, \n",
    "    )\n",
    "    print(\"Usando OpenRouter como modelo LLM.\")\n",
    "\n",
    "# Función para monitorear uso de memoria\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Uso de memoria actual: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nConfiguración de modelos completada.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbfb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando componentes del índice desde './storage_index'...\n",
      "Índice FAISS cargado correctamente.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage_index\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage_index\\index_store.json.\n",
      "¡Índice completo cargado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Creación o Carga PERSISTENTE del Índice Vectorial (Optimizada para Hugging Face Spaces)\n",
    "\n",
    "# 1. Definimos las rutas donde se guardarán los componentes del índice\n",
    "PERSIST_DIR = \"./storage_index\"\n",
    "FAISS_INDEX_PATH = os.path.join(PERSIST_DIR, \"faiss_index.bin\")\n",
    "DOCSTORE_PATH = os.path.join(PERSIST_DIR, \"docstore.json\")\n",
    "INDEX_STORE_PATH = os.path.join(PERSIST_DIR, \"index_store.json\")\n",
    "\n",
    "# Función para eliminar una carpeta y su contenido de forma segura\n",
    "def remove_directory_safely(directory_path):\n",
    "    import shutil\n",
    "    try:\n",
    "        if os.path.exists(directory_path):\n",
    "            print(f\"Eliminando directorio '{directory_path}'...\")\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directorio '{directory_path}' eliminado exitosamente.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al eliminar directorio: {e}\")\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    # Verificamos si existe el índice FAISS serializado\n",
    "    if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(DOCSTORE_PATH) and os.path.exists(INDEX_STORE_PATH):\n",
    "        print(f\"Cargando componentes del índice desde '{PERSIST_DIR}'...\")\n",
    "        try:\n",
    "            # 1. Cargamos el índice FAISS con su método nativo\n",
    "            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "            print(\"Índice FAISS cargado correctamente.\")\n",
    "            \n",
    "            # 2. Creamos el vector store y el storage context\n",
    "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                vector_store=vector_store,\n",
    "                persist_dir=PERSIST_DIR\n",
    "            )\n",
    "            \n",
    "            # 3. Cargamos el índice completo\n",
    "            metadata_index = load_index_from_storage(storage_context)\n",
    "            print(\"¡Índice completo cargado exitosamente!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar algún componente del índice: {e}\")\n",
    "            if remove_directory_safely(PERSIST_DIR):\n",
    "                print(\"Recreando el índice desde cero...\")\n",
    "            else:\n",
    "                raise Exception(\"No se pudo eliminar el índice corrupto. Elimínalo manualmente.\")\n",
    "    \n",
    "    # Si llegamos aquí, es porque necesitamos crear el índice (la carpeta no existe o se eliminó)\n",
    "    if not os.path.exists(PERSIST_DIR) or not os.path.exists(FAISS_INDEX_PATH):\n",
    "        print(\"Creando un nuevo índice...\")\n",
    "        \n",
    "        if df_metadatos is None:\n",
    "            print(\"No se puede crear el índice porque el DataFrame de metadatos no se cargó. Revisa la Celda 1.\")\n",
    "            metadata_index = None\n",
    "        else:\n",
    "            # --- Lógica de creación (la misma que tenías antes) ---\n",
    "            print(\"=\"*50)\n",
    "            print(\"INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\\nEste proceso solo se ejecutará una vez.\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            Settings.node_parser = SentenceSplitter(chunk_size=2048)\n",
    "\n",
    "            # La función optimize_text_for_embedding sigue siendo útil para el contenido de los nodos\n",
    "            def optimize_text_for_embedding(text, max_length=1500):\n",
    "                if not isinstance(text, str): return \"\"\n",
    "                text = text[:max_length]\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                return text.strip()\n",
    "                \n",
    "            print(\"Convirtiendo metadatos en documentos de LlamaIndex...\")\n",
    "            metadata_documents = []\n",
    "            columnas_a_incluir = [\n",
    "                'Año de publicación', 'Código sugerido', 'Nombre del archivo', 'Título del estudio', \n",
    "                'Categoría', 'Tipo de documento', 'Subcategoría', 'Idioma', 'Número de páginas', \n",
    "                'Incorpora perspectiva de género', 'Año de término', 'Lugar de término', \n",
    "                'Palabras clave', 'Objetivo', 'Metodología', 'Resumen', 'Documento público', \n",
    "                'Publicación destacada', 'Publicado', 'Editorial', 'Entidad solicitante',\n",
    "                'Entidad a cargo del estudio', 'Investigador u organismo principal', 'Equipo de investigación', \n",
    "                'Número de documento administrativo', 'Tipo de financiamiento', 'Costo del estudio', \n",
    "                'Base de datos enviada', 'Base pública', 'Url'\n",
    "            ]\n",
    "            columnas_prioritarias = ['Título del estudio', 'Resumen', 'Palabras clave', 'Objetivo', \n",
    "                                    'Metodología', 'Investigador u organismo principal']\n",
    "\n",
    "            for index, row in tqdm(df_metadatos.iterrows(), total=df_metadatos.shape[0], desc=\"Procesando metadatos\"):\n",
    "                partes_texto = []\n",
    "                for col in columnas_prioritarias:\n",
    "                    if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                        texto_optimizado = optimize_text_for_embedding(str(row[col]), 300)\n",
    "                        if texto_optimizado:\n",
    "                            partes_texto.append(f\"{col}: {texto_optimizado}\"); partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "                \n",
    "                for col in columnas_a_incluir:\n",
    "                    if col not in columnas_prioritarias:\n",
    "                        if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                            texto_optimizado = optimize_text_for_embedding(str(row[col]), 200)\n",
    "                            if texto_optimizado:\n",
    "                                partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "                \n",
    "                contenido_buscable = \". \".join(partes_texto)\n",
    "                doc = Document(\n",
    "                    text=contenido_buscable,\n",
    "                    metadata={col: optimize_text_for_embedding(str(row.get(col, '')), 500) for col in columnas_a_incluir}\n",
    "                )\n",
    "                metadata_documents.append(doc)\n",
    "            \n",
    "            print(\"Creando índice vectorial con FAISS...\")\n",
    "            d = len(Settings.embed_model.get_text_embedding(\"test\"))\n",
    "            faiss_index = faiss.IndexFlatL2(d)\n",
    "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "            \n",
    "            metadata_index = VectorStoreIndex.from_documents(\n",
    "                metadata_documents,\n",
    "                storage_context=storage_context,\n",
    "                show_progress=True \n",
    "            )\n",
    "            \n",
    "            # --- ¡PASOS CRUCIALES DE GUARDADO! ---\n",
    "            # 1. Creamos el directorio si no existe\n",
    "            os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "            \n",
    "            # 2. Guardamos el índice FAISS usando su método nativo\n",
    "            print(f\"Guardando el índice FAISS en '{FAISS_INDEX_PATH}'...\")\n",
    "            faiss.write_index(faiss_index, FAISS_INDEX_PATH)\n",
    "            \n",
    "            # 3. Guardamos el resto del storage context (docstore, index_store)\n",
    "            print(f\"Guardando los componentes de LlamaIndex en '{PERSIST_DIR}'...\")\n",
    "            metadata_index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "            print(\"¡ÍNDICE CREADO Y GUARDADO EN FORMATO COMPATIBLE!\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error no manejado: {e}\")\n",
    "    print(\"Por favor, borra manualmente la carpeta 'storage_index' e intenta nuevamente.\")\n",
    "    metadata_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lanzando la interfaz de CEMIA con botones...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 4: Lógica del Chatbot e Interfaz Gradio (Caja de Texto Limpia)\n",
    "\n",
    "if 'metadata_index' not in locals() or metadata_index is None or 'df_metadatos' not in locals() or df_metadatos is None:\n",
    "    print(\"ERROR: El índice o el DataFrame no han sido creados. Por favor, ejecuta las celdas anteriores correctamente.\")\n",
    "else:\n",
    "    # --- FUNCIONES DE APOYO (Sin cambios) ---\n",
    "    def normalize_text(text):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn').lower()\n",
    "\n",
    "    def get_initial_chat_state():\n",
    "        return {\"user_name\": None, \"current_mode\": \"waiting_for_name\", \"temp_author\": None,\n",
    "                \"last_query_term\": None, \"last_results\": [], \"current_page\": 0, \"last_search_type\": None}\n",
    "    \n",
    "    def search_by_topic(message, author_filter=None):\n",
    "        print(f\"Búsqueda semántica por tema: '{message}' con filtro de autor: {author_filter}\"); extraction_template = PromptTemplate(\"Reescribe la consulta del usuario en una lista de 3 a 5 términos de búsqueda concisos, separados por comas. Responde SÓLO con los términos.\\nConsulta: '{query_str}'\\nTérminos:\"); response = Settings.llm.complete(extraction_template.format(query_str=message)); llm_term = str(response).strip().lower().replace('*', '').replace('-', ''); print(f\"Término de búsqueda final: '{llm_term}'\"); retriever = metadata_index.as_retriever(similarity_top_k=50); retrieved_nodes = retriever.retrieve(llm_term); results = []\n",
    "        if retrieved_nodes:\n",
    "            normalized_author_filter_parts = normalize_text(author_filter).split() if author_filter else []\n",
    "            for node_with_score in retrieved_nodes:\n",
    "                if normalized_author_filter_parts:\n",
    "                    full_text_norm = normalize_text(node_with_score.node.text)\n",
    "                    if not all(part in full_text_norm for part in normalized_author_filter_parts): continue\n",
    "                meta = node_with_score.node.metadata\n",
    "                results.append({'Título del estudio': meta.get('Título del estudio', 'N/A'), 'Autor(es)': f\"{meta.get('Investigador u organismo principal', 'N/A')}, {meta.get('Equipo de investigación', 'N/A')}\", 'Año de publicación': meta.get('Año de publicación', 'N/A'), 'Url': meta.get('Url', 'No disponible'), '__is_semantic': True, 'score': node_with_score.score})\n",
    "        results.sort(key=lambda x: x['score'], reverse=True); relevant_results = [res for res in results if res[\"score\"] >= 0.40]\n",
    "        return {\"results\": relevant_results, \"query_term\": llm_term}\n",
    "    \n",
    "    def search_by_author(authors_query):\n",
    "        print(f\"Búsqueda por autor/es: '{authors_query}'.\"); author_groups = [group.strip() for group in authors_query.split(',') if group.strip()]\n",
    "        def df_to_results_list(df):\n",
    "            df_copy = df.copy(); df_copy.loc[:, 'Autor(es)'] = df_copy.apply(lambda row: f\"{row.get('Investigador u organismo principal', 'N/A')}, {row.get('Equipo de investigación', 'N/A')}\", axis=1); return df_copy.to_dict('records')\n",
    "        and_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "        for group in author_groups:\n",
    "            normalized_group_parts = normalize_text(group).split(); group_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "            for part in normalized_group_parts: group_mask &= (df_metadatos['principal_norm'].str.contains(part, na=False) | df_metadatos['equipo_norm'].str.contains(part, na=False))\n",
    "            and_mask &= group_mask\n",
    "        and_hits_df = df_metadatos[and_mask].copy()\n",
    "        if not and_hits_df.empty:\n",
    "            and_hits_df.loc[:, 'Año de publicación num'] = pd.to_numeric(and_hits_df['Año de publicación'], errors='coerce').fillna(0); sorted_df = and_hits_df.sort_values(by='Año de publicación num', ascending=False); results_list = df_to_results_list(sorted_df); header = f\"He encontrado {len(results_list)} documento(s) que incluyen a '{authors_query}'.\"; return {\"results\": results_list, \"header\": header}\n",
    "        if len(author_groups) > 1:\n",
    "            response_parts = [f\"No he encontrado documentos que incluyan a todos estos autores juntos. A continuación, los resultados para cada uno por separado:\\n\"]; any_found = False\n",
    "            for group in author_groups:\n",
    "                normalized_group_parts = normalize_text(group).split(); group_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "                for part in normalized_group_parts: group_mask &= (df_metadatos['principal_norm'].str.contains(part, na=False) | df_metadatos['equipo_norm'].str.contains(part, na=False))\n",
    "                individual_hits_df = df_metadatos[group_mask].copy()\n",
    "                if not individual_hits_df.empty:\n",
    "                    any_found = True; total_individual = len(individual_hits_df); limit_text = f\"(mostrando los 5 más recientes de {total_individual})\" if total_individual > 5 else \"\"; response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}' {limit_text}:\\n\"); individual_hits_df.loc[:, 'Año de publicación num'] = pd.to_numeric(individual_hits_df['Año de publicación'], errors='coerce').fillna(0); sorted_individual_df = individual_hits_df.sort_values(by='Año de publicación num', ascending=False).head(5)\n",
    "                    for i, (_, row) in enumerate(sorted_individual_df.iterrows(), 1): response_parts.append(f\"{i}. **Título:** {row.get('Título del estudio', 'N/A')}\\n   - **Autor(es):** {row.get('Investigador u organismo principal', 'N/A')}, {row.get('Equipo de investigación', 'N/A')}\\n   - **Año:** {row.get('Año de publicación', 'N/A')}\\n   - **URL:** {row.get('Url', 'No disponible')}\\n\")\n",
    "                else: response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}':\\n\\nNo se encontraron documentos.\")\n",
    "            if any_found: return {\"results\": [], \"header\": \"\\n\".join(response_parts), \"is_special_format\": True}\n",
    "        return {\"results\": [], \"header\": f\"No he encontrado documentos de '{authors_query}'. Revisa la ortografía.\"}\n",
    "    \n",
    "    def format_results_page(state):\n",
    "        total_found = len(state[\"last_results\"]);\n",
    "        if total_found == 0: return state.get(\"custom_header\") or \"No se encontraron resultados.\"\n",
    "        MAX_TO_SHOW = 5; start_index = (state[\"current_page\"] - 1) * MAX_TO_SHOW; nodes_to_show = state[\"last_results\"][start_index : start_index + MAX_TO_SHOW]\n",
    "        if not nodes_to_show: return \"No hay más documentos que mostrar. Ya estás en la última página.\"\n",
    "        header_parts = [state.get(\"custom_header\", f\"Mostrando resultados para '{state['last_query_term']}'\")]\n",
    "        if total_found > MAX_TO_SHOW: header_parts.append(f\"(página {state['current_page']} de {-(-total_found // MAX_TO_SHOW)}, mostrando {len(nodes_to_show)} de {total_found} total)\")\n",
    "        header = \" \".join(filter(None, header_parts)) + \":\\n\\n\"; lista_formateada = []\n",
    "        for i, res in enumerate(nodes_to_show, start=start_index + 1):\n",
    "            titulo = res.get('Título del estudio', 'N/A'); autores = res.get('Autor(es)', 'N/A'); ano = str(res.get('Año de publicación', 'N/A')); url = res.get('Url', 'No disponible')\n",
    "            if res.get('__is_semantic'): score = res.get('score', 0); relevance_label = \"Muy Alta\" if score >= 0.75 else \"Alta\" if score >= 0.5 else \"Media\"; relevance_str = f\"**(Relevancia: {relevance_label} [{score:.0%}])**\"; lista_formateada.append(f\"{i}. **Título:** {titulo} {relevance_str}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}\")\n",
    "            else: lista_formateada.append(f\"{i}. **Título:** {titulo}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}\")\n",
    "        footer = f\"\\n\\n--- \\n*Para ver más, escribe **siguiente** o `página {state['current_page'] + 1}`.*\" if (start_index + MAX_TO_SHOW) < total_found else \"\"\n",
    "        return header + \"\\n\\n\".join(lista_formateada) + footer\n",
    "    \n",
    "    def extract_name_from_greeting(message):\n",
    "        patterns = [r\"(?:soy|me llamo|mi nombre es|ll[aá]mame|dime)\\s+(.+)\", r\"^(?:hola,? soy|hola,? mi nombre es)\\s+(.+)\"];\n",
    "        for pattern in patterns: match = re.search(pattern, message, re.IGNORECASE);\n",
    "        if match: return match.group(1).strip().title()\n",
    "        return message.strip().title()\n",
    "\n",
    "    with gr.Blocks(theme=\"default\") as demo:\n",
    "        chat_state = gr.State(get_initial_chat_state())\n",
    "        chatbot = gr.Chatbot(value=[{\"role\": \"assistant\", \"content\": \"¡Hola! Soy CemIA, asistente de IA del Centro de Estudios del MINEDUC. ¿Cómo quieres que te llame?\"}], type=\"messages\", label=\"CEMIA\", height=500)\n",
    "        with gr.Row(visible=False) as button_row:\n",
    "            btn_author = gr.Button(\"Búsqueda por autoría\"); btn_topic = gr.Button(\"Búsqueda por temática\"); btn_combo = gr.Button(\"Búsqueda por autoría y temática\"); btn_bye = gr.Button(\"Adiós\")\n",
    "        user_input = gr.Textbox(placeholder=\"Escribe tu nombre para comenzar...\", label=\"Tu respuesta\", autofocus=True)\n",
    "\n",
    "        def make_buttons_interactive(interactive=True):\n",
    "            return gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive)\n",
    "\n",
    "        def handle_user_input(message, history, current_state):\n",
    "            history.append({\"role\": \"user\", \"content\": message}); mode = current_state[\"current_mode\"]\n",
    "            \n",
    "            message_lower = message.strip().lower()\n",
    "            if message_lower in [\"siguiente\", \"más\", \"mas\"] or re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower):\n",
    "                if current_state[\"last_results\"]:\n",
    "                    page_match = re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower); current_state[\"current_page\"] = int(page_match.group(1)) if page_match else current_state[\"current_page\"] + 1\n",
    "                    history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
    "                else: history.append({\"role\": \"assistant\", \"content\": \"No hay resultados anteriores para paginar. Por favor, realiza una nueva búsqueda.\"})\n",
    "                return history, current_state, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "\n",
    "            button_row_update, user_input_update = gr.update(), gr.update(value=\"\", interactive=False, placeholder=\"Elige una opción de los botones\");\n",
    "            btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update = make_buttons_interactive(False); response_text = \"\"\n",
    "\n",
    "            if mode == \"waiting_for_name\":\n",
    "                user_name = extract_name_from_greeting(message); current_state[\"user_name\"], current_state[\"current_mode\"] = user_name, \"waiting_for_choice\"\n",
    "                response_text = f\"¡Qué bueno tenerte aquí, {user_name}! Por favor, elige una opción.\"; button_row_update, (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = gr.update(visible=True), make_buttons_interactive(True)\n",
    "            elif mode == \"waiting_for_author\":\n",
    "                search_data = search_by_author(message)\n",
    "                if search_data.get(\"is_special_format\"): response_text = search_data[\"header\"]; current_state[\"last_results\"] = []\n",
    "                else: current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": message, \"current_page\": 1, \"custom_header\": search_data[\"header\"], \"last_search_type\": \"author\"}); response_text = format_results_page(current_state)\n",
    "                current_state[\"current_mode\"] = \"waiting_for_choice\"; (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True); user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "            elif mode == \"waiting_for_topic\":\n",
    "                search_data = search_by_topic(message); current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": search_data[\"query_term\"], \"current_page\": 1, \"last_search_type\": \"topic\", \"current_mode\": \"waiting_for_choice\"}); response_text = format_results_page(current_state)\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True); user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "            elif mode == \"waiting_for_combo_author\":\n",
    "                current_state[\"temp_author\"], current_state[\"current_mode\"] = message.strip(), \"waiting_for_combo_topic\"; response_text = \"Entendido. Ahora dime la temática que quieres buscar.\"; user_input_update = gr.update(value=\"\", interactive=True, placeholder=\"Escribe la temática aquí...\")\n",
    "            elif mode == \"waiting_for_combo_topic\":\n",
    "                author = current_state[\"temp_author\"]; search_data = search_by_topic(message, author_filter=author); current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": search_data[\"query_term\"], \"current_page\": 1, \"custom_header\": f\"Resultados para '{search_data['query_term']}' filtrados por el autor '{author}'\", \"last_search_type\": \"topic\", \"current_mode\": \"waiting_for_choice\", \"temp_author\": None}); response_text = format_results_page(current_state)\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True); user_input_update = gr.update(value=\"\", placeholder=\"Escribe 'siguiente' o elige una opción\", interactive=True)\n",
    "            else: response_text = \"Por favor, utiliza uno de los botones para continuar.\"; (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "            \n",
    "            history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "            return history, current_state, button_row_update, btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update, user_input_update\n",
    "\n",
    "        def handle_choice(history, current_state, choice):\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Has elegido: **{choice}**\"}); prompt_message, mode = \"\", current_state[\"current_mode\"]\n",
    "            if choice == \"Búsqueda por autoría\": mode, prompt_message = \"waiting_for_author\", \"Por favor, escribe el nombre del autor/a o autores/as (separados por comas).\"\n",
    "            elif choice == \"Búsqueda por temática\": mode, prompt_message = \"waiting_for_topic\", \"Por favor, escribe la temática que te interesa.\"\n",
    "            elif choice == \"Búsqueda por autoría y temática\": mode, prompt_message = \"waiting_for_combo_author\", \"Perfecto. Primero, escribe el nombre del autor.\"\n",
    "            current_state[\"current_mode\"] = mode; history.append({\"role\": \"assistant\", \"content\": prompt_message}); b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            \n",
    "            # --- CAMBIO AQUÍ ---\n",
    "            return history, current_state, b1, b2, b3, b4, gr.update(value=\"\", interactive=True, placeholder=\"Escribe aquí...\")\n",
    "\n",
    "        def handle_bye(current_state):\n",
    "            if current_state[\"user_name\"]:\n",
    "                gr.Info(f\"Adiós {current_state['user_name']}, ¡recuerda que siempre estoy disponible para asistirte!\")\n",
    "                current_state[\"current_mode\"] = \"ended\"\n",
    "            \n",
    "            b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            ui_input = gr.update(value=\"\", interactive=False, placeholder=\"Conversación finalizada. Recarga la página para empezar de nuevo.\")\n",
    "            return current_state, b1, b2, b3, b4, ui_input\n",
    "\n",
    "        user_input.submit(handle_user_input, [user_input, chatbot, chat_state], [chatbot, chat_state, button_row, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_author.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_topic.click(lambda h, s: handle_choice(h, s, \"Búsqueda por temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_combo.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría y temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_bye.click(fn=handle_bye, inputs=[chat_state], outputs=[chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "\n",
    "    print(\"\\nLanzando la interfaz de CEMIA con botones...\")\n",
    "    demo.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
