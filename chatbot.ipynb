{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Importaciones\n",
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import re\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import gc  # Para gestión explícita de memoria\n",
    "import psutil  # Para monitoreo de recursos del sistema\n",
    "\n",
    "# LlamaIndex Core y componentes modernos\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "# Utilidades para optimización\n",
    "from tqdm.auto import tqdm  # Para barras de progreso mejoradas\n",
    "\n",
    "# Importamos las librerías necesarias para el caching\n",
    "from joblib import Memory\n",
    "\n",
    "# Interfaz\n",
    "import gradio as gr\n",
    "\n",
    "# Configurar threads para bibliotecas de cálculo numérico\n",
    "num_cores = psutil.cpu_count(logical=True)\n",
    "os.environ['OMP_NUM_THREADS'] = str(num_cores)  # OpenMP\n",
    "os.environ['MKL_NUM_THREADS'] = str(num_cores)  # Intel MKL si está disponible\n",
    "\n",
    "print(f\"Librerías importadas. ¡Entorno listo con {num_cores} núcleos disponibles!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc27b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Celda 2: CONFIGURACIÓN OPTIMIZADA PARA CPU ---\n",
    "\n",
    "# Cargar la clave de API desde el archivo .env\n",
    "load_dotenv()\n",
    "if \"OPENROUTER_API_KEY\" in os.environ:\n",
    "    print(\"Clave de API de OpenRouter cargada.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró la clave de API de OpenRouter.\")\n",
    "\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    print(\"Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró el token de Hugging Face.\")\n",
    "\n",
    "# --- Configuración global de LlamaIndex optimizada para CPU --\n",
    "\n",
    "# Determinar tamaño óptimo de batch según número de núcleos\n",
    "# Regla empírica: 8-16 vectores por núcleo para balance óptimo\n",
    "optimal_batch_size = max(64, num_cores * 8)\n",
    "\n",
    "print(f\"Configurando el modelo de embeddings con batch size óptimo de {optimal_batch_size} para {num_cores} núcleos.\")\n",
    "\n",
    "# Configuración optimizada para CPU - usando modelo más eficiente\n",
    "# Nota: SentenceTransformer no acepta pooling_strategy como parámetro directo\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # Modelo más pequeño pero eficiente\n",
    "    embed_batch_size=optimal_batch_size,   # Tamaño de lote optimizado para tu CPU\n",
    "    cache_folder=\"./model_cache\",          # Guardar modelo en caché para cargas más rápidas\n",
    "    normalize=True,                        # Normalizar embeddings para mejor rendimiento\n",
    "    trust_remote_code=True                 # Permitir código remoto para optimizaciones\n",
    ")\n",
    "\n",
    "# Selecciona el proveedor de LLM aquí: \"openrouter\" o \"gemini\"\n",
    "LLM_PROVIDER = \"openrouter\"  # Cambia a \"gemini\" si prefieres usar Google Gemini\n",
    "\n",
    "if LLM_PROVIDER == \"gemini\":\n",
    "    Settings.llm = GoogleGenAI(\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Usando Gemini como modelo LLM.\")\n",
    "else:\n",
    "    Settings.llm = OpenRouter(\n",
    "        model=\"google/gemma-3n-e4b-it:free\",\n",
    "        #model=\"google/gemma-3n-e2b-it:free\",\n",
    "        #model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        #model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        #model=\"deepseek/deepseek-r1-0528:free\",\n",
    "        #model=\"deepseek/deepseek-chat:free\",\n",
    "        #model=\"google/gemini-2.0-flash-exp:free\",\n",
    "        #model=\"mistralai/mistral-nemo:free\",\n",
    "        #model=\"qwen/qwq-32b:free\",\n",
    "        #model=\"microsoft/mai-ds-r1:free\",\n",
    "        #model=\"meta-llama/llama-4-maverick:free\",\n",
    "        temperature=0.1, \n",
    "    )\n",
    "    print(\"Usando OpenRouter como modelo LLM.\")\n",
    "\n",
    "# --- Definición de Rutas ---\n",
    "METADATA_FILE = os.path.join(\"data\", \"metadatos_chatbot_final.csv\")\n",
    "PERSIST_DIR = \"./storage_index\"\n",
    "\n",
    "# Función para monitorear uso de memoria\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Uso de memoria actual: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nConfiguración de modelos y rutas completada.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NUEVA CELDA 3: Carga Limpia de Metadatos ---\n",
    "\n",
    "def load_metadata_dataframe(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Llenamos los valores vacíos con \"No especificado\" para evitar errores\n",
    "        df.fillna(\"No especificado\", inplace=True)\n",
    "        print(\"Metadatos cargados y procesados.\")\n",
    "        display(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al cargar metadatos: {e}.\")\n",
    "        return None\n",
    "\n",
    "df_metadatos = load_metadata_dataframe(METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELDA 4 (VERSIÓN FINAL CON CACHING Y SCOPE CORREGIDO) ---\n",
    "\n",
    "# 1. Configuramos la ubicación de la caché\n",
    "CACHE_DIR = \"./joblib_cache\"\n",
    "memory = Memory(CACHE_DIR, verbose=1)\n",
    "\n",
    "# 2. Envolvemos la lógica en una función que ACEPTA el dataframe como argumento\n",
    "@memory.cache\n",
    "def get_or_create_index(dataframe): ### CAMBIO 1: Añadido 'dataframe' como argumento\n",
    "    print(\"=\"*50)\n",
    "    print(\"INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Comprobamos el argumento, no una variable global\n",
    "    if dataframe is None: ### CAMBIO 2: Usamos el argumento 'dataframe'\n",
    "        print(\"No se puede crear el índice porque el DataFrame proporcionado es nulo.\")\n",
    "        return None\n",
    "\n",
    "    # El resto del código usa el 'dataframe' que le pasamos como argumento\n",
    "    Settings.node_parser = SentenceSplitter(chunk_size=2048)\n",
    "\n",
    "    def optimize_text_for_embedding(text, max_length=1500):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        text = text[:max_length]; text = re.sub(r'\\\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "        \n",
    "    print(\"Convirtiendo metadatos en documentos de LlamaIndex...\")\n",
    "    metadata_documents = []\n",
    "    columnas_a_incluir = [\n",
    "        'Año de publicación', 'Código sugerido', 'Nombre del archivo', 'Título del estudio', \n",
    "        'Categoría', 'Tipo de documento', 'Subcategoría', 'Idioma', 'Número de páginas', \n",
    "        'Incorpora perspectiva de género', 'Año de término', 'Lugar de término', \n",
    "        'Palabras clave', 'Objetivo', 'Metodología', 'Resumen', 'Documento público', \n",
    "        'Publicación destacada', 'Publicado', 'Editorial', 'Entidad solicitante',\n",
    "        'Entidad a cargo del estudio', 'Investigador u organismo principal', 'Equipo de investigación', \n",
    "        'Número de documento administrativo', 'Tipo de financiamiento', 'Costo del estudio', \n",
    "        'Base de datos enviada', 'Base pública', 'Url'\n",
    "    ]\n",
    "    columnas_prioritarias = ['Título del estudio', 'Resumen', 'Palabras clave', 'Objetivo', \n",
    "                             'Metodología', 'Investigador u organismo principal']\n",
    "\n",
    "    for index, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0], desc=\"Procesando metadatos\"):\n",
    "        partes_texto = []\n",
    "        for col in columnas_prioritarias:\n",
    "            if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                texto_optimizado = optimize_text_for_embedding(str(row[col]), 300)\n",
    "                if texto_optimizado:\n",
    "                    partes_texto.append(f\"{col}: {texto_optimizado}\"); partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "        \n",
    "        for col in columnas_a_incluir:\n",
    "            if col not in columnas_prioritarias:\n",
    "                if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                    texto_optimizado = optimize_text_for_embedding(str(row[col]), 200)\n",
    "                    if texto_optimizado:\n",
    "                        partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "        \n",
    "        contenido_buscable = \". \".join(partes_texto)\n",
    "        doc = Document(\n",
    "            text=contenido_buscable,\n",
    "            metadata={col: optimize_text_for_embedding(str(row.get(col, '')), 500) for col in columnas_a_incluir}\n",
    "        )\n",
    "        metadata_documents.append(doc)\n",
    "    \n",
    "    print(\"Creando índice vectorial con FAISS...\")\n",
    "    d = len(Settings.embed_model.get_text_embedding(\"test\"))\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        metadata_documents,\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True \n",
    "    )\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"¡ÍNDICE CREADO Y GUARDADO EN CACHÉ EXITOSAMENTE!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# 3. LLAMAMOS A LA FUNCIÓN PASÁNDOLE EL DATAFRAME\n",
    "if 'df_metadatos' in locals() and df_metadatos is not None:\n",
    "    # Le pasamos df_metadatos como argumento\n",
    "    metadata_index = get_or_create_index(df_metadatos) ### CAMBIO 3: Pasamos el dataframe a la función\n",
    "else:\n",
    "    print(\"No se puede crear el índice porque el DataFrame de metadatos no se cargó. Ejecuta la celda 3.\")\n",
    "    metadata_index = None # Asegurarnos de que la variable exista como None si falla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELDA EJECUCION (VERSIÓN CEMIA CON ROUTER DE IA AVANZADO) ---\n",
    "\n",
    "if 'metadata_index' not in locals() or metadata_index is None or 'df_metadatos' not in locals():\n",
    "    print(\"ERROR: El índice o el DataFrame no han sido creados. Por favor, ejecuta las celdas anteriores.\")\n",
    "else:\n",
    "    # --- Pre-cálculo de autores conocidos (sin cambios) ---\n",
    "    autores_principales = set(df_metadatos['Investigador u organismo principal'].str.lower().unique())\n",
    "    autores_equipo_raw = set(df_metadatos['Equipo de investigación'].str.lower().unique())\n",
    "    autores_equipo = set()\n",
    "    for item in autores_equipo_raw:\n",
    "        if isinstance(item, str):\n",
    "            nombres = re.split(r'[;,]', item)\n",
    "            for nombre in nombres:\n",
    "                if nombre.strip():\n",
    "                    autores_equipo.add(nombre.strip())\n",
    "    KNOWN_AUTHORS = autores_principales.union(autores_equipo)\n",
    "    print(f\"Se identificaron {len(KNOWN_AUTHORS)} autores únicos para la búsqueda precisa.\")\n",
    "\n",
    "    # --- GESTOR DE ESTADO DE LA CONVERSACIÓN (sin cambios) ---\n",
    "    chat_state = {\"last_query_term\": None, \"last_results\": [], \"last_author_filter\": None, \"current_page\": 0}\n",
    "\n",
    "    # --- Funciones de apoyo (sin cambios) ---\n",
    "    def get_relevance_label(score):\n",
    "        if score >= 0.75 and score <= 1 : return \"Muy Alta\"\n",
    "        elif score >= 0.5 and score < 0.75 : return \"Alta\"\n",
    "        elif score >= 0.25 and score < 0.5: return \"Media\"\n",
    "        else: return \"Baja\"\n",
    "    \n",
    "    def find_relevance_reason(node_text, search_terms_str):\n",
    "        search_terms = [term.strip() for term in search_terms_str.split(',') if term.strip()]\n",
    "        reasons = []\n",
    "        for term in search_terms:\n",
    "            match = re.search(re.escape(term), node_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                start, end = match.span(); context_start = max(0, start - 50); context_end = min(len(node_text), end + 70)\n",
    "                context_snippet = node_text[context_start:context_end].strip().replace(match.group(0), f\"**{match.group(0)}**\")\n",
    "                text_before_match = node_text[:start]\n",
    "                metadata_match = re.findall(r\"(\\b[\\w\\s'óáéíúñü]+\\b):\\s\", text_before_match)\n",
    "                metadata_name = metadata_match[-1] if metadata_match else \"Contenido\"\n",
    "                reasons.append(f\"Término '{term}' encontrado en **{metadata_name}** (contexto: *'...{context_snippet}...'*)\")\n",
    "        if not reasons: return f\"Relevancia semántica general con '{search_terms_str}'\"\n",
    "        return \". \".join(reasons)\n",
    "\n",
    "    def search_by_author(author_name):\n",
    "        print(f\"Búsqueda precisa por autor: '{author_name}'.\")\n",
    "        # Hacemos una búsqueda simple de subcadena, que es robusta\n",
    "        mask = (df_metadatos['Investigador u organismo principal'].str.contains(author_name, case=False, na=False, regex=False) |\n",
    "                df_metadatos['Equipo de investigación'].str.contains(author_name, case=False, na=False, regex=False))\n",
    "        author_hits_df = df_metadatos[mask].copy()\n",
    "        author_hits_df['Año de publicación num'] = pd.to_numeric(author_hits_df['Año de publicación'], errors='coerce').fillna(0)\n",
    "        sorted_hits_df = author_hits_df.sort_values(by='Año de publicación num', ascending=False)\n",
    "        num_docs_encontrados = len(sorted_hits_df)\n",
    "        if num_docs_encontrados == 0: return f\"No he encontrado documentos del autor '{author_name}'. Prueba con otro nombre o solo el apellido.\"\n",
    "        header = f\"He encontrado {num_docs_encontrados} documento(s) del autor '{author_name}', ordenados por año (del más reciente al más antiguo):\\n\\n\"\n",
    "        lista_formateada = []\n",
    "        for i, (index, row) in enumerate(sorted_hits_df.iterrows(), 1):\n",
    "            titulo, autores, ano_str, url = row.get('Título del estudio', 'No especificado'), f\"{row.get('Investigador u organismo principal', 'No especificado')}, {row.get('Equipo de investigación', 'No especificado')}\", str(row.get('Año de publicación', 'No especificado')), row.get('Url', 'No disponible')\n",
    "            try: ano = str(int(float(ano_str)))\n",
    "            except (ValueError, TypeError): ano = ano_str\n",
    "            info_doc = (f\"{i}. **Título:** {titulo}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}\")\n",
    "            lista_formateada.append(info_doc)\n",
    "        return header + \"\\n\\n\".join(lista_formateada)\n",
    "\n",
    "    def format_results_page():\n",
    "        total_found, current_page, llm_term = len(chat_state[\"last_results\"]), chat_state[\"current_page\"], chat_state[\"last_query_term\"]\n",
    "        author_context = f\" del autor '{chat_state['last_author_filter']}'\" if chat_state['last_author_filter'] else \"\"\n",
    "        if total_found == 0: return f\"No he encontrado documentos relevantes sobre '{llm_term}'. Por favor, intenta reformular tu búsqueda.\"\n",
    "        MAX_TO_SHOW, start_index = 5, (current_page - 1) * 5\n",
    "        nodes_to_show = chat_state[\"last_results\"][start_index:start_index + MAX_TO_SHOW]\n",
    "        if not nodes_to_show: return f\"No hay más documentos que mostrar sobre '{llm_term}'. Ya estás en la última página.\"\n",
    "        header = \"\"\n",
    "        if current_page == 1:\n",
    "            if total_found > MAX_TO_SHOW: header = f\"He encontrado un total de {total_found} documentos relevantes sobre '{llm_term}'{author_context}. Mostrando los {len(nodes_to_show)} más relevantes (página {current_page}):\\n\\n\"\n",
    "            else: header = f\"He encontrado {total_found} documento(s) relevante(s) sobre '{llm_term}'{author_context}:\\n\\n\"\n",
    "        else: header = f\"Mostrando la página {current_page} de {-( -total_found // MAX_TO_SHOW)} para '{llm_term}' (documentos {start_index + 1}-{start_index + len(nodes_to_show)}):\\n\\n\"\n",
    "        lista_formateada = []\n",
    "        for i, res in enumerate(nodes_to_show, start=start_index + 1):\n",
    "            metadata, score, reason_text = res['node'].metadata, res['score'], res['reason']\n",
    "            relevance_label = get_relevance_label(score); relevance_percentage = f\"{score:.0%}\"; url = metadata.get('Url', 'No disponible')\n",
    "            info_doc = (f\"{i}. **Título:** {metadata.get('Título del estudio', 'No especificado')} **(Relevancia: {relevance_label} [{relevance_percentage}])**\\n\"\n",
    "                        f\"   - **Autor(es):** {metadata.get('Investigador u organismo principal', 'No especificado')}, {metadata.get('Equipo de investigación', 'No especificado')}\\n\"\n",
    "                        f\"   - **Año:** {metadata.get('Año de publicación', 'No especificado')}\\n\"\n",
    "                        f\"   - **URL:** {url}\\n\"\n",
    "                        f\"   - ***Motivo:*** *{reason_text}*\")\n",
    "            lista_formateada.append(info_doc)\n",
    "        footer = \"\"\n",
    "        if (start_index + MAX_TO_SHOW) < total_found: footer = f\"\\n\\n--- \\n*Para ver más, escribe 'siguiente', 'más' o 'página {current_page + 1}'.*\"\n",
    "        return header + \"\\n\\n\".join(lista_formateada) + footer\n",
    "\n",
    "    def search_by_topic(message, author_filter=None):\n",
    "        print(f\"Búsqueda semántica por tema: '{message}'.\")\n",
    "        print(\"Extrayendo concepto clave con LLM...\")\n",
    "        extraction_template = PromptTemplate(\n",
    "            \"Eres un motor de búsqueda semántica. Tu objetivo es reescribir la pregunta de un usuario en una consulta optimizada que capture la intención principal para encontrar documentos relevantes en una base de datos académica.\\n\"\n",
    "            \"La consulta reescrita debe ser una frase o una lista de conceptos clave concisa y directa, separados por comas.\\n\"\n",
    "            \"Ejemplo 1: 'estudios sobre brecha de genero del autor perez' -> brecha de género, desigualdad de género en la educación\\n\"\n",
    "            \"Ejemplo 2: 'documentos sobre evaluacion docente' -> evaluación docente, desempeño de profesores\\n\"\n",
    "            \"---\\nPregunta original: '{query_str}'\\nConsulta optimizada:\"\n",
    "        )\n",
    "        prompt_final = extraction_template.format(query_str=message)\n",
    "        response = Settings.llm.complete(prompt_final)\n",
    "        llm_term = str(response).strip().lower()\n",
    "        print(f\"Término de búsqueda final: '{llm_term}'\")\n",
    "        retriever = metadata_index.as_retriever(similarity_top_k=30)\n",
    "        retrieved_nodes = retriever.retrieve(llm_term)\n",
    "        RELEVANCE_THRESHOLD = 0.40; relevant_results = []\n",
    "        if retrieved_nodes:\n",
    "            all_potential_results = []\n",
    "            for node_with_score in retrieved_nodes:\n",
    "                node = node_with_score.node\n",
    "                if author_filter:\n",
    "                    autor_principal = node.metadata.get('Investigador u organismo principal', '').lower()\n",
    "                    equipo = node.metadata.get('Equipo de investigación', '').lower()\n",
    "                    if author_filter.lower() not in autor_principal and author_filter.lower() not in equipo: continue\n",
    "                detailed_reason = find_relevance_reason(node.text, llm_term)\n",
    "                all_potential_results.append({\"node\": node, \"score\": node_with_score.score, \"reason\": detailed_reason})\n",
    "            if all_potential_results:\n",
    "                all_potential_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "                for result in all_potential_results:\n",
    "                    if result[\"score\"] >= RELEVANCE_THRESHOLD: relevant_results.append(result)\n",
    "        chat_state[\"last_query_term\"] = llm_term; chat_state[\"last_results\"] = relevant_results\n",
    "        chat_state[\"last_author_filter\"] = author_filter; chat_state[\"current_page\"] = 1\n",
    "        return format_results_page()\n",
    "\n",
    "    # --- ROUTER DE IA Y GESTORES DE INTENCIONES ---\n",
    "    def get_user_intent(user_message):\n",
    "        intent_classifier_prompt = PromptTemplate(\n",
    "            \"Tu tarea es clasificar la intención del usuario en una de las siguientes categorías:\\n\"\n",
    "            \"- 'saludo'\\n- 'ayuda'\\n- 'chitchat'\\n- 'paginacion'\\n- 'busqueda_por_autor'\\n- 'busqueda_por_tema'\\n\\n\"\n",
    "            \"Ejemplos:\\n\"\n",
    "            \"Usuario: 'Hola' -> saludo\\n\"\n",
    "            \"Usuario: 'qué puedes hacer?' -> ayuda\\n\"\n",
    "            \"Usuario: 'gracias' -> chitchat\\n\"\n",
    "            \"Usuario: 'siguiente' -> paginacion\\n\"\n",
    "            \"Usuario: 'trabajos del autor Fernández' -> busqueda_por_autor\\n\"\n",
    "            \"Usuario: 'estudios sobre deserción' -> busqueda_por_tema\\n\"\n",
    "            \"Usuario: 'documentos de Bravo sobre mercado laboral' -> busqueda_por_autor\\n\\n\" # Ejemplo clave de búsqueda mixta\n",
    "            \"Analiza la siguiente pregunta y responde ÚNICAMENTE con el nombre de la categoría en minúsculas.\\n\\n\"\n",
    "            \"Pregunta del usuario: '{query_str}'\\nCategoría:\"\n",
    "        )\n",
    "        prompt = intent_classifier_prompt.format(query_str=user_message)\n",
    "        response = Settings.llm.complete(prompt)\n",
    "        intent = str(response).strip().lower().replace(\" \", \"_\")\n",
    "        print(f\"Intención detectada por la IA: '{intent}'\")\n",
    "        return intent\n",
    "\n",
    "    def extract_author_name(user_message):\n",
    "        name_extractor_prompt = PromptTemplate(\n",
    "            \"Tu tarea es extraer el nombre completo o apellido de un autor de la siguiente pregunta. Responde ÚNICAMENTE con el nombre.\\n\"\n",
    "            \"Ejemplo 1: 'trabajos del autor Bravo' -> Bravo\\n\"\n",
    "            \"Ejemplo 2: 'documentos de Carolina Fernández sobre currículo' -> Carolina Fernández\\n\"\n",
    "            \"---\\nPregunta del usuario: '{query_str}'\\nNombre del autor:\"\n",
    "        )\n",
    "        prompt = name_extractor_prompt.format(query_str=user_message)\n",
    "        response = Settings.llm.complete(prompt)\n",
    "        author_name = str(response).strip()\n",
    "        print(f\"Nombre de autor extraído por la IA: '{author_name}'\")\n",
    "        return author_name\n",
    "\n",
    "    def handle_chitchat(user_message):\n",
    "        chitchat_prompt = PromptTemplate(\n",
    "            \"Tu nombre es CEMIA, eres un asistente de IA sin género determinado del Centro de Estudios del MINEDUC. Eres profesional y amable. \"\n",
    "            \"Responde de forma breve a la pregunta conversacional del usuario. Luego, siempre redirige la conversación a tu función principal de buscar documentos.\\n\"\n",
    "            \"Ejemplo 1:\\nUsuario: '¿Cómo estás?'\\nRespuesta: ¡Funcionando a pleno rendimiento y disponible para ayudarte! ¿Qué documento te gustaría encontrar hoy?\\n\"\n",
    "            \"Ejemplo 2:\\nUsuario: 'gracias'\\nRespuesta: ¡De nada! Me alegra poder ayudar. ¿Hay algo más que necesites buscar?\\n\"\n",
    "            \"---\\nPregunta del usuario: '{chitchat_query}'\\nRespuesta:\"\n",
    "        )\n",
    "        prompt = chitchat_prompt.format(chitchat_query=user_message)\n",
    "        response = Settings.llm.complete(prompt)\n",
    "        return str(response).strip()\n",
    "\n",
    "    # --- FUNCIÓN PRINCIPAL CON ROUTER DE IA AVANZADO ---\n",
    "    def unified_search(message, history):\n",
    "        print(\"-\" * 50); print(f\"Recibida consulta: '{message}'\")\n",
    "        \n",
    "        intent = get_user_intent(message)\n",
    "\n",
    "        if intent == \"saludo\":\n",
    "            return \"¡Hola! Soy CEMIA, tu asistente de IA para el Centro de Estudios del MINEDUC. Mi función es ayudarte a encontrar documentos e investigaciones. Puedes preguntarme por temas o autores. ¿En qué puedo asistirte hoy?\"\n",
    "\n",
    "        elif intent == \"ayuda\":\n",
    "            return \"\"\"\n",
    "            ¡Por supuesto! Soy CEMIA y te explico cómo puedo ayudarte:\n",
    "            **1. Búsqueda por Tema:** `Estudios sobre retención en primer año`\n",
    "            **2. Búsqueda por Autor:** `trabajos del autor Fernández`\n",
    "            **3. Búsqueda Combinada:** `documentos de Bravo sobre mercado laboral`\n",
    "            **4. Navegación:** `siguiente` o `página 3` para ver más resultados.\n",
    "            ¿Qué te gustaría buscar?\n",
    "            \"\"\"\n",
    "        elif intent == \"chitchat\":\n",
    "            return handle_chitchat(message)\n",
    "\n",
    "        elif intent == \"paginacion\":\n",
    "            if not chat_state[\"last_results\"]: return \"Primero necesitas hacer una búsqueda para poder ver más resultados.\"\n",
    "            query_lower = message.strip().lower()\n",
    "            page_match = re.match(r\"p[aá]gina\\s+(\\d+)\", query_lower)\n",
    "            if page_match: chat_state[\"current_page\"] = int(page_match.group(1))\n",
    "            else: chat_state[\"current_page\"] += 1\n",
    "            return format_results_page()\n",
    "        \n",
    "        elif intent == \"busqueda_por_autor\":\n",
    "            chat_state.clear(); chat_state.update({\"last_query_term\": None, \"last_results\": [], \"last_author_filter\": None, \"current_page\": 0})\n",
    "            author_name = extract_author_name(message)\n",
    "            return search_by_author(author_name)\n",
    "\n",
    "        else: # Intención 'busqueda_por_tema' o cualquier otra por defecto\n",
    "            chat_state.clear(); chat_state.update({\"last_query_term\": None, \"last_results\": [], \"last_author_filter\": None, \"current_page\": 0})\n",
    "            return search_by_topic(message)\n",
    "\n",
    "    # --- Creación de la Interfaz de Gradio (con nueva identidad) ---\n",
    "    chat_interface = gr.ChatInterface(\n",
    "        fn=unified_search,\n",
    "        chatbot=gr.Chatbot(height=500, type=\"messages\"),\n",
    "        type=\"messages\",\n",
    "        title=\"CEMIA (Asistente IA del Centro de Estudios)\",\n",
    "        description=\"Busca por tema, autor o ambos. El chatbot recordará tu última búsqueda para que puedas pedirle 'siguiente' o 'página 2'.\",\n",
    "        examples=[\"¿qué puedes hacer?\", \"estudios sobre brecha de género\", \"trabajos de Bravo\", \"siguiente\"],\n",
    "        theme=\"default\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nLanzando la interfaz de CEMIA...\")\n",
    "    chat_interface.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
