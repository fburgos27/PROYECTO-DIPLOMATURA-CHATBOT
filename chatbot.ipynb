{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebbc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas. ¡Entorno listo con 6 núcleos disponibles!\n",
      "Metadatos cargados y procesados.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Año de publicación</th>\n",
       "      <th>Código sugerido</th>\n",
       "      <th>Nombre del archivo</th>\n",
       "      <th>Título del estudio</th>\n",
       "      <th>Categoría</th>\n",
       "      <th>Tipo de documento</th>\n",
       "      <th>Subcategoría</th>\n",
       "      <th>Idioma</th>\n",
       "      <th>Número de páginas</th>\n",
       "      <th>Incorpora perspectiva de género</th>\n",
       "      <th>...</th>\n",
       "      <th>Entidad solicitante</th>\n",
       "      <th>Entidad a cargo del estudio</th>\n",
       "      <th>Investigador u organismo principal</th>\n",
       "      <th>Equipo de investigación</th>\n",
       "      <th>Número de documento administrativo</th>\n",
       "      <th>Tipo de financiamiento</th>\n",
       "      <th>Costo del estudio</th>\n",
       "      <th>Base de datos enviada</th>\n",
       "      <th>Base pública</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>E99-0000</td>\n",
       "      <td>Mercado Laboral en Docencia</td>\n",
       "      <td>Análisis del Mercado de Servicios de Docencia ...</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Informe</td>\n",
       "      <td>Docentes</td>\n",
       "      <td>Español</td>\n",
       "      <td>123</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Ministerio de Educación de Chile; MINEDUC</td>\n",
       "      <td>Facultad de Ciencias Económicas y Administrati...</td>\n",
       "      <td>Bravo, David</td>\n",
       "      <td>Ruiz-Tagle, Jaime; Sanhueza, Ricardo</td>\n",
       "      <td>Sin información</td>\n",
       "      <td>Interno</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>https://bibliotecadigital.mineduc.cl/bitstream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>E00-0001</td>\n",
       "      <td>Evaluación Resultados ENLACES</td>\n",
       "      <td>Diseño de un modelo de evaluación de resultado...</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Informe</td>\n",
       "      <td>TIC</td>\n",
       "      <td>Español</td>\n",
       "      <td>53</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Enlaces; Ministerio de Educación de Chile; MIN...</td>\n",
       "      <td>Asesorías para el Desarrollo</td>\n",
       "      <td>Raczynski, Dagmar</td>\n",
       "      <td>Pavez, M. Angélica</td>\n",
       "      <td>Sin información</td>\n",
       "      <td>Interno</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>https://bibliotecadigital.mineduc.cl/bitstream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>E01-0001</td>\n",
       "      <td>Focalización Becas Liceos Para Todos CC</td>\n",
       "      <td>Focalización de Becas del Programa Liceo para ...</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Informe</td>\n",
       "      <td>Monitoreo y evaluación</td>\n",
       "      <td>Español</td>\n",
       "      <td>30</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Ministerio de Educación de Chile; MINEDUC</td>\n",
       "      <td>Departamento de Salud Pública, PUC</td>\n",
       "      <td>Marshall, Guillermo</td>\n",
       "      <td>Correa, Lorena</td>\n",
       "      <td>Sin información</td>\n",
       "      <td>Interno</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>https://bibliotecadigital.mineduc.cl/bitstream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>E01-0002</td>\n",
       "      <td>Focalización Becas Liceos Para Todos ECO</td>\n",
       "      <td>Focalización de Becas del Programa Liceo para ...</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Informe</td>\n",
       "      <td>Monitoreo y evaluación</td>\n",
       "      <td>Español</td>\n",
       "      <td>61</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Ministerio de Educación de Chile; MINEDUC</td>\n",
       "      <td>Departamento de Salud Pública, PUC</td>\n",
       "      <td>Marshall, Guillermo</td>\n",
       "      <td>Correa, Lorena</td>\n",
       "      <td>Sin información</td>\n",
       "      <td>Interno</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>https://bibliotecadigital.mineduc.cl/bitstream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>E01-0003</td>\n",
       "      <td>OFT Educación Media</td>\n",
       "      <td>Objetivos Fundamentales Transversales en la En...</td>\n",
       "      <td>Estudios</td>\n",
       "      <td>Informe</td>\n",
       "      <td>Currículo</td>\n",
       "      <td>Español</td>\n",
       "      <td>61</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Ministerio de Educación de Chile; MINEDUC</td>\n",
       "      <td>UCE, MINEDUC</td>\n",
       "      <td>Fernández, Carolina</td>\n",
       "      <td>Jashes, Jessana</td>\n",
       "      <td>Sin información</td>\n",
       "      <td>Interno</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>https://bibliotecadigital.mineduc.cl/bitstream...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Año de publicación Código sugerido  \\\n",
       "0                1999        E99-0000   \n",
       "1                2000        E00-0001   \n",
       "2                2001        E01-0001   \n",
       "3                2001        E01-0002   \n",
       "4                2001        E01-0003   \n",
       "\n",
       "                         Nombre del archivo  \\\n",
       "0               Mercado Laboral en Docencia   \n",
       "1             Evaluación Resultados ENLACES   \n",
       "2   Focalización Becas Liceos Para Todos CC   \n",
       "3  Focalización Becas Liceos Para Todos ECO   \n",
       "4                       OFT Educación Media   \n",
       "\n",
       "                                  Título del estudio Categoría  \\\n",
       "0  Análisis del Mercado de Servicios de Docencia ...  Estudios   \n",
       "1  Diseño de un modelo de evaluación de resultado...  Estudios   \n",
       "2  Focalización de Becas del Programa Liceo para ...  Estudios   \n",
       "3  Focalización de Becas del Programa Liceo para ...  Estudios   \n",
       "4  Objetivos Fundamentales Transversales en la En...  Estudios   \n",
       "\n",
       "  Tipo de documento            Subcategoría   Idioma  Número de páginas  \\\n",
       "0           Informe                Docentes  Español                123   \n",
       "1           Informe                     TIC  Español                 53   \n",
       "2           Informe  Monitoreo y evaluación  Español                 30   \n",
       "3           Informe  Monitoreo y evaluación  Español                 61   \n",
       "4           Informe               Currículo  Español                 61   \n",
       "\n",
       "  Incorpora perspectiva de género  ...  \\\n",
       "0                              No  ...   \n",
       "1                              No  ...   \n",
       "2                              No  ...   \n",
       "3                              No  ...   \n",
       "4                              No  ...   \n",
       "\n",
       "                                 Entidad solicitante  \\\n",
       "0          Ministerio de Educación de Chile; MINEDUC   \n",
       "1  Enlaces; Ministerio de Educación de Chile; MIN...   \n",
       "2          Ministerio de Educación de Chile; MINEDUC   \n",
       "3          Ministerio de Educación de Chile; MINEDUC   \n",
       "4          Ministerio de Educación de Chile; MINEDUC   \n",
       "\n",
       "                         Entidad a cargo del estudio  \\\n",
       "0  Facultad de Ciencias Económicas y Administrati...   \n",
       "1                       Asesorías para el Desarrollo   \n",
       "2                 Departamento de Salud Pública, PUC   \n",
       "3                 Departamento de Salud Pública, PUC   \n",
       "4                                       UCE, MINEDUC   \n",
       "\n",
       "  Investigador u organismo principal               Equipo de investigación  \\\n",
       "0                       Bravo, David  Ruiz-Tagle, Jaime; Sanhueza, Ricardo   \n",
       "1                  Raczynski, Dagmar                    Pavez, M. Angélica   \n",
       "2                Marshall, Guillermo                        Correa, Lorena   \n",
       "3                Marshall, Guillermo                        Correa, Lorena   \n",
       "4                Fernández, Carolina                       Jashes, Jessana   \n",
       "\n",
       "  Número de documento administrativo Tipo de financiamiento Costo del estudio  \\\n",
       "0                    Sin información                Interno                 0   \n",
       "1                    Sin información                Interno                 0   \n",
       "2                    Sin información                Interno                 0   \n",
       "3                    Sin información                Interno                 0   \n",
       "4                    Sin información                Interno                 0   \n",
       "\n",
       "  Base de datos enviada Base pública  \\\n",
       "0                    No           No   \n",
       "1                    No           No   \n",
       "2                    No           No   \n",
       "3                    No           No   \n",
       "4                    No           No   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://bibliotecadigital.mineduc.cl/bitstream...  \n",
       "1  https://bibliotecadigital.mineduc.cl/bitstream...  \n",
       "2  https://bibliotecadigital.mineduc.cl/bitstream...  \n",
       "3  https://bibliotecadigital.mineduc.cl/bitstream...  \n",
       "4  https://bibliotecadigital.mineduc.cl/bitstream...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando columnas normalizadas para la búsqueda de autores (insensible a acentos)...\n",
      "Columnas normalizadas creadas exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: Importaciones, Carga de Datos y Configuración Inicial (Actualizada)\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import psutil\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "# No necesitamos joblib, ya que usaremos la persistencia de LlamaIndex\n",
    "import gradio as gr\n",
    "from IPython.display import display # Importado para display(df.head())\n",
    "import unicodedata # <--- AÑADIDO PARA MANEJAR ACENTOS\n",
    "\n",
    "# LlamaIndex Core y componentes\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    Document,\n",
    "    load_index_from_storage # <--- AÑADIDO PARA CARGAR EL ÍNDICE\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. Configurar Entorno ---\n",
    "num_cores = psutil.cpu_count(logical=True)\n",
    "os.environ['OMP_NUM_THREADS'] = str(num_cores)\n",
    "os.environ['MKL_NUM_THREADS'] = str(num_cores)\n",
    "print(f\"Librerías importadas. ¡Entorno listo con {num_cores} núcleos disponibles!\")\n",
    "\n",
    "# --- 2. Definir Rutas, Funciones de Normalización y Carga de Metadatos ---\n",
    "METADATA_FILE = os.path.join(\"data\", \"metadatos_chatbot_final.csv\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Convierte texto a minúsculas y elimina acentos.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Descompone los caracteres con acentos en caracter base + acento\n",
    "    # y luego elimina los caracteres de acento (categoría 'Mn')\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    ).lower()\n",
    "\n",
    "def load_metadata_dataframe(filepath):\n",
    "    \"\"\"Carga los metadatos desde un CSV a un DataFrame de Pandas.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.fillna(\"No especificado\", inplace=True)\n",
    "        print(\"Metadatos cargados y procesados.\")\n",
    "        display(df.head())\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo de metadatos en la ruta: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al cargar metadatos: {e}.\")\n",
    "        return None\n",
    "\n",
    "# Cargar el DataFrame al inicio\n",
    "df_metadatos = load_metadata_dataframe(METADATA_FILE)\n",
    "\n",
    "# --- CAMBIO CLAVE: Pre-normalizar columnas de autores para búsqueda eficiente ---\n",
    "if df_metadatos is not None:\n",
    "    print(\"Creando columnas normalizadas para la búsqueda de autores (insensible a acentos)...\")\n",
    "    df_metadatos['principal_norm'] = df_metadatos['Investigador u organismo principal'].apply(normalize_text)\n",
    "    df_metadatos['equipo_norm'] = df_metadatos['Equipo de investigación'].apply(normalize_text)\n",
    "    print(\"Columnas normalizadas creadas exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc27b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API de OpenRouter cargada.\n",
      "Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\n",
      "Configurando el modelo de embeddings con batch size óptimo de 64 para 6 núcleos.\n",
      "Usando Gemini como modelo LLM.\n",
      "\n",
      "Configuración de modelos completada.\n",
      "Uso de memoria actual: 552.16 MB\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: CONFIGURACIÓN OPTIMIZADA DE MODELOS DE IA\n",
    "\n",
    "# Cargar la clave de API desde el archivo .env\n",
    "load_dotenv()\n",
    "if \"OPENROUTER_API_KEY\" in os.environ:\n",
    "    print(\"Clave de API de OpenRouter cargada.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró la clave de API de OpenRouter.\")\n",
    "\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    print(\"Token de Hugging Face (HF_TOKEN) encontrado en el entorno.\")\n",
    "else:\n",
    "    print(\"¡ADVERTENCIA! No se encontró el token de Hugging Face.\")\n",
    "\n",
    "# --- Configuración global de LlamaIndex optimizada para CPU ---\n",
    "optimal_batch_size = max(64, num_cores * 8)\n",
    "print(f\"Configurando el modelo de embeddings con batch size óptimo de {optimal_batch_size} para {num_cores} núcleos.\")\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    embed_batch_size=optimal_batch_size,\n",
    "    cache_folder=\"./model_cache\",\n",
    "    normalize=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Selecciona el proveedor de LLM aquí: \"openrouter\" o \"gemini\"\n",
    "LLM_PROVIDER = \"gemini\"\n",
    "\n",
    "if LLM_PROVIDER == \"gemini\":\n",
    "    Settings.llm = GoogleGenAI(\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    print(\"Usando Gemini como modelo LLM.\")\n",
    "else:\n",
    "    Settings.llm = OpenRouter(\n",
    "        model=\"google/gemma-3n-e4b-it:free\",\n",
    "        #model=\"google/gemma-3n-e2b-it:free\",\n",
    "        #model=\"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        #model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        #model=\"deepseek/deepseek-r1-0528:free\",\n",
    "        #model=\"deepseek/deepseek-chat:free\",\n",
    "        #model=\"google/gemini-2.0-flash-exp:free\",\n",
    "        #model=\"mistralai/mistral-nemo:free\",\n",
    "        #model=\"qwen/qwq-32b:free\",\n",
    "        #model=\"microsoft/mai-ds-r1:free\",\n",
    "        #model=\"meta-llama/llama-4-maverick:free\",\n",
    "        temperature=0.1, \n",
    "    )\n",
    "    print(\"Usando OpenRouter como modelo LLM.\")\n",
    "\n",
    "# Función para monitorear uso de memoria\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Uso de memoria actual: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nConfiguración de modelos completada.\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbfb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontró un índice existente. Creando uno nuevo...\n",
      "==================================================\n",
      "INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\n",
      "Este proceso solo se ejecutará una vez.\n",
      "==================================================\n",
      "Convirtiendo metadatos en documentos de LlamaIndex...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac84fa6c758345d2adf78ffaea703446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando metadatos:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando índice vectorial con FAISS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed85d207f63742cbbf7c696e8bcc29c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7894acdd484815b0d8bd6b3c13e4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el índice en './storage_index' para uso futuro...\n",
      "==================================================\n",
      "¡ÍNDICE CREADO Y GUARDADO PERMANENTEMENTE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Creación o Carga PERSISTENTE del Índice Vectorial\n",
    "\n",
    "# 1. Definimos la ruta donde se guardará el índice de forma permanente.\n",
    "PERSIST_DIR = \"./storage_index\"\n",
    "\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "    # Si la carpeta ya existe, cargamos el índice directamente desde ella.\n",
    "    print(f\"Cargando índice existente desde '{PERSIST_DIR}'...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    metadata_index = load_index_from_storage(storage_context)\n",
    "    print(\"¡Índice cargado exitosamente!\")\n",
    "\n",
    "else:\n",
    "    # Si la carpeta NO existe, ejecutamos el proceso de creación por primera y única vez.\n",
    "    print(\"No se encontró un índice existente. Creando uno nuevo...\")\n",
    "    \n",
    "    if df_metadatos is None:\n",
    "        print(\"No se puede crear el índice porque el DataFrame de metadatos no se cargó. Revisa la Celda 1.\")\n",
    "        metadata_index = None\n",
    "    else:\n",
    "        # --- Lógica de creación (la misma que tenías antes) ---\n",
    "        print(\"=\"*50)\n",
    "        print(\"INICIANDO PROCESO DE CREACIÓN DE ÍNDICE (LENTO LA PRIMERA VEZ)...\\nEste proceso solo se ejecutará una vez.\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        Settings.node_parser = SentenceSplitter(chunk_size=2048)\n",
    "\n",
    "        # La función optimize_text_for_embedding sigue siendo útil para el contenido de los nodos\n",
    "        def optimize_text_for_embedding(text, max_length=1500):\n",
    "            if not isinstance(text, str): return \"\"\n",
    "            text = text[:max_length]\n",
    "            text = re.sub(r'\\s+', ' ', text)  # Corregido: eliminé una barra invertida extra\n",
    "            return text.strip()\n",
    "            \n",
    "        print(\"Convirtiendo metadatos en documentos de LlamaIndex...\")\n",
    "        metadata_documents = []\n",
    "        columnas_a_incluir = [\n",
    "            'Año de publicación', 'Código sugerido', 'Nombre del archivo', 'Título del estudio', \n",
    "            'Categoría', 'Tipo de documento', 'Subcategoría', 'Idioma', 'Número de páginas', \n",
    "            'Incorpora perspectiva de género', 'Año de término', 'Lugar de término', \n",
    "            'Palabras clave', 'Objetivo', 'Metodología', 'Resumen', 'Documento público', \n",
    "            'Publicación destacada', 'Publicado', 'Editorial', 'Entidad solicitante',\n",
    "            'Entidad a cargo del estudio', 'Investigador u organismo principal', 'Equipo de investigación', \n",
    "            'Número de documento administrativo', 'Tipo de financiamiento', 'Costo del estudio', \n",
    "            'Base de datos enviada', 'Base pública', 'Url'\n",
    "        ]\n",
    "        columnas_prioritarias = ['Título del estudio', 'Resumen', 'Palabras clave', 'Objetivo', \n",
    "                                'Metodología', 'Investigador u organismo principal']\n",
    "\n",
    "        for index, row in tqdm(df_metadatos.iterrows(), total=df_metadatos.shape[0], desc=\"Procesando metadatos\"):\n",
    "            partes_texto = []\n",
    "            for col in columnas_prioritarias:\n",
    "                if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                    texto_optimizado = optimize_text_for_embedding(str(row[col]), 300)\n",
    "                    if texto_optimizado:\n",
    "                        partes_texto.append(f\"{col}: {texto_optimizado}\"); partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "            \n",
    "            for col in columnas_a_incluir:\n",
    "                if col not in columnas_prioritarias:\n",
    "                    if col in row and pd.notna(row[col]) and row[col] != \"No especificado\":\n",
    "                        texto_optimizado = optimize_text_for_embedding(str(row[col]), 200)\n",
    "                        if texto_optimizado:\n",
    "                            partes_texto.append(f\"{col}: {texto_optimizado}\")\n",
    "            \n",
    "            contenido_buscable = \". \".join(partes_texto)\n",
    "            doc = Document(\n",
    "                text=contenido_buscable,\n",
    "                metadata={col: optimize_text_for_embedding(str(row.get(col, '')), 500) for col in columnas_a_incluir}\n",
    "            )\n",
    "            metadata_documents.append(doc)\n",
    "        \n",
    "        print(\"Creando índice vectorial con FAISS...\")\n",
    "        d = len(Settings.embed_model.get_text_embedding(\"test\"))\n",
    "        faiss_index = faiss.IndexFlatL2(d)\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        metadata_index = VectorStoreIndex.from_documents(\n",
    "            metadata_documents,\n",
    "            storage_context=storage_context,\n",
    "            show_progress=True \n",
    "        )\n",
    "        \n",
    "        # --- ¡PASO CRUCIAL! ---\n",
    "        # 2. Guardamos el índice creado en la carpeta definida.\n",
    "        print(f\"Guardando el índice en '{PERSIST_DIR}' para uso futuro...\")\n",
    "        metadata_index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"¡ÍNDICE CREADO Y GUARDADO PERMANENTEMENTE!\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lanzando la interfaz de CEMIA con botones...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Búsqueda semántica por tema: 'hay documentos sobre enseñanza básica?' con filtro de autor: None\n",
      "Término de búsqueda final: 'conceptos:\n",
      "\n",
      "* **educación básica:**  (o educación primaria, educación elemental, dependiendo del contexto geográfico)\n",
      "* **documentación:** (o  publicaciones, artículos, libros, informes, etc.)\n",
      "* **enseñanza:** (o pedagogía, didáctica, instrucción)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\fburg\\Videos\\Proyecto-Diplomatura-ChatBot\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\fburg\\AppData\\Local\\Temp\\ipykernel_22148\\2759400171.py\", line 188, in handle_user_input\n",
      "    history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
      "                                                    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fburg\\AppData\\Local\\Temp\\ipykernel_22148\\2759400171.py\", line 106, in format_results_page\n",
      "    header = \" \".join(header_parts) + \":\\n\\n\"\n",
      "             ~~~~~~~~^^^^^^^^^^^^^^\n",
      "TypeError: sequence item 0: expected str instance, NoneType found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Búsqueda por autor/es: 'fabian burgos'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fburg\\AppData\\Local\\Temp\\ipykernel_22148\\2759400171.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  and_hits_df['Año de publicación num'] = pd.to_numeric(and_hits_df['Año de publicación'], errors='coerce').fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Lógica del Chatbot e Interfaz Gradio (Con formato messages)\n",
    "\n",
    "if 'metadata_index' not in locals() or metadata_index is None or 'df_metadatos' not in locals() or df_metadatos is None:\n",
    "    print(\"ERROR: El índice o el DataFrame no han sido creados. Por favor, ejecuta las celdas anteriores correctamente.\")\n",
    "else:\n",
    "    # --- GESTOR DE ESTADO Y FUNCIONES DE APOYO ---\n",
    "    def get_initial_chat_state():\n",
    "        return {\n",
    "            \"user_name\": None, \"current_mode\": \"waiting_for_name\", \"temp_author\": None,\n",
    "            \"last_query_term\": None, \"last_results\": [], \"current_page\": 0, \"last_search_type\": None\n",
    "        }\n",
    "\n",
    "    def search_by_topic(message, author_filter=None):\n",
    "        print(f\"Búsqueda semántica por tema: '{message}' con filtro de autor: {author_filter}\")\n",
    "        extraction_template = PromptTemplate(\"Reescribe la consulta del usuario en una lista de conceptos clave para una base de datos académica.\\nConsulta: '{query_str}'\\nConceptos:\")\n",
    "        response = Settings.llm.complete(extraction_template.format(query_str=message))\n",
    "        llm_term = str(response).strip().lower()\n",
    "        print(f\"Término de búsqueda final: '{llm_term}'\")\n",
    "        retriever = metadata_index.as_retriever(similarity_top_k=50)\n",
    "        retrieved_nodes = retriever.retrieve(llm_term)\n",
    "        results = []\n",
    "        if retrieved_nodes:\n",
    "            normalized_author_filter_parts = normalize_text(author_filter).split() if author_filter else []\n",
    "            for node_with_score in retrieved_nodes:\n",
    "                if normalized_author_filter_parts:\n",
    "                    full_text_norm = normalize_text(node_with_score.node.text)\n",
    "                    if not all(part in full_text_norm for part in normalized_author_filter_parts):\n",
    "                        continue\n",
    "                meta = node_with_score.node.metadata\n",
    "                results.append({'Título del estudio': meta.get('Título del estudio', 'N/A'),\n",
    "                                'Autor(es)': f\"{meta.get('Investigador u organismo principal', 'N/A')}, {meta.get('Equipo de investigación', 'N/A')}\",\n",
    "                                'Año de publicación': meta.get('Año de publicación', 'N/A'), 'Url': meta.get('Url', 'No disponible'),\n",
    "                                '__is_semantic': True, 'score': node_with_score.score})\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        relevant_results = [res for res in results if res[\"score\"] >= 0.40]\n",
    "        return {\"results\": relevant_results, \"query_term\": llm_term}\n",
    "\n",
    "    def search_by_author(authors_query):\n",
    "        print(f\"Búsqueda por autor/es: '{authors_query}'.\")\n",
    "        author_groups = [group.strip() for group in authors_query.split(',') if group.strip()]\n",
    "        \n",
    "        def df_to_results_list(df):\n",
    "            df_copy = df.copy()\n",
    "            df_copy['Autor(es)'] = df_copy.apply(lambda row: f\"{row.get('Investigador u organismo principal', 'N/A')}, {row.get('Equipo de investigación', 'N/A')}\", axis=1)\n",
    "            return df_copy.to_dict('records')\n",
    "\n",
    "        # Etapa 1: Búsqueda AND\n",
    "        and_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "        for group in author_groups:\n",
    "            normalized_group_parts = normalize_text(group).split()\n",
    "            group_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "            for part in normalized_group_parts:\n",
    "                group_mask &= (df_metadatos['principal_norm'].str.contains(part, na=False) |\n",
    "                               df_metadatos['equipo_norm'].str.contains(part, na=False))\n",
    "            and_mask &= group_mask\n",
    "        \n",
    "        and_hits_df = df_metadatos[and_mask]\n",
    "        if not and_hits_df.empty:\n",
    "            and_hits_df['Año de publicación num'] = pd.to_numeric(and_hits_df['Año de publicación'], errors='coerce').fillna(0)\n",
    "            sorted_df = and_hits_df.sort_values(by='Año de publicación num', ascending=False)\n",
    "            results_list = df_to_results_list(sorted_df)\n",
    "            header = f\"He encontrado {len(results_list)} documento(s) que incluyen a '{authors_query}'.\"\n",
    "            return {\"results\": results_list, \"header\": header}\n",
    "\n",
    "        # Etapa 2: Fallback a búsqueda OR individual\n",
    "        if len(author_groups) > 1:\n",
    "            response_parts = [f\"No he encontrado documentos que incluyan a todos estos autores juntos. A continuación, los resultados para cada uno por separado:\\n\"]\n",
    "            any_found = False\n",
    "            for group in author_groups:\n",
    "                normalized_group_parts = normalize_text(group).split()\n",
    "                group_mask = pd.Series([True] * len(df_metadatos), index=df_metadatos.index)\n",
    "                for part in normalized_group_parts:\n",
    "                    group_mask &= (df_metadatos['principal_norm'].str.contains(part, na=False) |\n",
    "                                   df_metadatos['equipo_norm'].str.contains(part, na=False))\n",
    "                \n",
    "                individual_hits_df = df_metadatos[group_mask]\n",
    "                \n",
    "                if not individual_hits_df.empty:\n",
    "                    any_found = True\n",
    "                    total_individual = len(individual_hits_df)\n",
    "                    limit_text = f\"(mostrando los 5 más recientes de {total_individual})\" if total_individual > 5 else \"\"\n",
    "                    response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}' {limit_text}:\\n\")\n",
    "                    individual_hits_df['Año de publicación num'] = pd.to_numeric(individual_hits_df['Año de publicación'], errors='coerce').fillna(0)\n",
    "                    sorted_individual_df = individual_hits_df.sort_values(by='Año de publicación num', ascending=False).head(5)\n",
    "                    for i, (_, row) in enumerate(sorted_individual_df.iterrows(), 1):\n",
    "                        response_parts.append(f\"{i}. **Título:** {row.get('Título del estudio', 'N/A')}\\n   - **Autor(es):** {row.get('Investigador u organismo principal', 'N/A')}, {row.get('Equipo de investigación', 'N/A')}\\n   - **Año:** {row.get('Año de publicación', 'N/A')}\\n   - **URL:** {row.get('Url', 'No disponible')}\\n\")\n",
    "                else:\n",
    "                    response_parts.append(f\"\\n---\\n\\n### Documentos de '{group}':\\n\\nNo se encontraron documentos.\")\n",
    "            \n",
    "            if any_found:\n",
    "                 return {\"results\": [], \"header\": \"\\n\".join(response_parts), \"is_special_format\": True}\n",
    "\n",
    "        return {\"results\": [], \"header\": f\"No he encontrado documentos de '{authors_query}'. Revisa la ortografía.\"}\n",
    "\n",
    "    def format_results_page(state):\n",
    "        total_found = len(state[\"last_results\"])\n",
    "        if total_found == 0: return state.get(\"custom_header\") or \"No se encontraron resultados.\"\n",
    "        MAX_TO_SHOW = 5\n",
    "        start_index = (state[\"current_page\"] - 1) * MAX_TO_SHOW\n",
    "        nodes_to_show = state[\"last_results\"][start_index : start_index + MAX_TO_SHOW]\n",
    "        if not nodes_to_show: return \"No hay más documentos que mostrar. Ya estás en la última página.\"\n",
    "\n",
    "        header_parts = [state.get(\"custom_header\", f\"Mostrando resultados para '{state['last_query_term']}'\")]\n",
    "        if total_found > MAX_TO_SHOW:\n",
    "            header_parts.append(f\"(página {state['current_page']} de {-(-total_found // MAX_TO_SHOW)}, mostrando {len(nodes_to_show)} de {total_found} total)\")\n",
    "        header = \" \".join(header_parts) + \":\\n\\n\"\n",
    "        \n",
    "        lista_formateada = []\n",
    "        for i, res in enumerate(nodes_to_show, start=start_index + 1):\n",
    "            titulo = res.get('Título del estudio', 'N/A'); autores = res.get('Autor(es)', 'N/A'); ano = str(res.get('Año de publicación', 'N/A')); url = res.get('Url', 'No disponible')\n",
    "            if res.get('__is_semantic'):\n",
    "                score = res.get('score', 0); relevance_label = \"Muy Alta\" if score >= 0.75 else \"Alta\" if score >= 0.5 else \"Media\"\n",
    "                relevance_str = f\"**(Relevancia: {relevance_label} [{score:.0%}])**\"\n",
    "                lista_formateada.append(f\"{i}. **Título:** {titulo} {relevance_str}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}\")\n",
    "            else:\n",
    "                 lista_formateada.append(f\"{i}. **Título:** {titulo}\\n   - **Autor(es):** {autores}\\n   - **Año:** {ano}\\n   - **URL:** {url}\")\n",
    "\n",
    "        footer = f\"\\n\\n--- \\n*Para ver más, escribe **siguiente** o `página {state['current_page'] + 1}`.*\" if (start_index + MAX_TO_SHOW) < total_found else \"\"\n",
    "        return header + \"\\n\\n\".join(lista_formateada) + footer\n",
    "    \n",
    "    def extract_name_from_greeting(message):\n",
    "        patterns = [r\"(?:soy|me llamo|mi nombre es|ll[aá]mame|dime)\\s+(.+)\", r\"^(?:hola,? soy|hola,? mi nombre es)\\s+(.+)\"]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, message, re.IGNORECASE)\n",
    "            if match: return match.group(1).strip().title()\n",
    "        return message.strip().title()\n",
    "\n",
    "    with gr.Blocks(theme=\"default\") as demo:\n",
    "        chat_state = gr.State(get_initial_chat_state())\n",
    "        chatbot = gr.Chatbot(\n",
    "            value=[{\"role\": \"assistant\", \"content\": \"¡Hola! Soy CemIA, asistente de IA del Centro de Estudios del MINEDUC. ¿Cómo quieres que te llame?\"}], \n",
    "            type=\"messages\",\n",
    "            label=\"CEMIA\", \n",
    "            height=500\n",
    "        )\n",
    "        with gr.Row(visible=False) as button_row:\n",
    "            btn_author = gr.Button(\"Búsqueda por autoría\")\n",
    "            btn_topic = gr.Button(\"Búsqueda por temática\")\n",
    "            btn_combo = gr.Button(\"Búsqueda por autoría y temática\")\n",
    "            btn_bye = gr.Button(\"Adiós\")\n",
    "        user_input = gr.Textbox(placeholder=\"Escribe tu nombre para comenzar...\", label=\"Tu respuesta\", autofocus=True)\n",
    "\n",
    "        def make_buttons_interactive(interactive=True):\n",
    "            return gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive), gr.update(interactive=interactive)\n",
    "\n",
    "        def handle_user_input(message, history, current_state):\n",
    "            # Añadir mensaje del usuario (para el formato 'messages')\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            mode = current_state[\"current_mode\"]\n",
    "            \n",
    "            message_lower = message.strip().lower()\n",
    "            if message_lower in [\"siguiente\", \"más\", \"mas\"] or re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower):\n",
    "                if current_state[\"last_results\"]:\n",
    "                    page_match = re.search(r\"p[aá]gina\\s+(\\d+)\", message_lower)\n",
    "                    current_state[\"current_page\"] = int(page_match.group(1)) if page_match else current_state[\"current_page\"] + 1\n",
    "                    response_content = format_results_page(current_state)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "                else:\n",
    "                    history.append({\"role\": \"assistant\", \"content\": \"No hay resultados anteriores para paginar. Por favor, realiza una nueva búsqueda.\"})\n",
    "                return history, current_state, gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "\n",
    "            button_row_update, user_input_update = gr.update(), gr.update(interactive=False, placeholder=\"Elige una opción de los botones\")\n",
    "            btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update = make_buttons_interactive(False)\n",
    "\n",
    "            if mode == \"waiting_for_name\":\n",
    "                user_name = extract_name_from_greeting(message)\n",
    "                current_state[\"user_name\"], current_state[\"current_mode\"] = user_name, \"waiting_for_choice\"\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"¡Qué bueno tenerte aquí, {user_name}! Por favor, elige una opción.\"})\n",
    "                button_row_update, (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = gr.update(visible=True), make_buttons_interactive(True)\n",
    "            \n",
    "            elif mode == \"waiting_for_author\":\n",
    "                search_data = search_by_author(message)\n",
    "                if search_data.get(\"is_special_format\"):\n",
    "                    history.append({\"role\": \"assistant\", \"content\": search_data[\"header\"]})\n",
    "                    current_state[\"last_results\"] = []\n",
    "                else:\n",
    "                    current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": message, \"current_page\": 1, \n",
    "                                          \"custom_header\": search_data[\"header\"], \"last_search_type\": \"author\"})\n",
    "                    history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
    "                current_state[\"current_mode\"] = \"waiting_for_choice\"\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "\n",
    "            elif mode == \"waiting_for_topic\":\n",
    "                search_data = search_by_topic(message)\n",
    "                current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": search_data[\"query_term\"], \"current_page\": 1,\n",
    "                                      \"custom_header\": None, \"last_search_type\": \"topic\", \"current_mode\": \"waiting_for_choice\"})\n",
    "                history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "            \n",
    "            elif mode == \"waiting_for_combo_author\":\n",
    "                current_state[\"temp_author\"], current_state[\"current_mode\"] = message.strip(), \"waiting_for_combo_topic\"\n",
    "                history.append({\"role\": \"assistant\", \"content\": \"Entendido. Ahora dime la temática que quieres buscar.\"})\n",
    "                user_input_update = gr.update(interactive=True, placeholder=\"Escribe la temática aquí...\")\n",
    "            \n",
    "            elif mode == \"waiting_for_combo_topic\":\n",
    "                author = current_state[\"temp_author\"]\n",
    "                search_data = search_by_topic(message, author_filter=author)\n",
    "                current_state.update({\"last_results\": search_data[\"results\"], \"last_query_term\": search_data[\"query_term\"], \"current_page\": 1,\n",
    "                                      \"custom_header\": f\"Resultados para '{search_data['query_term']}' filtrados por el autor '{author}'\", \n",
    "                                      \"last_search_type\": \"topic\", \"current_mode\": \"waiting_for_choice\", \"temp_author\": None})\n",
    "                history.append({\"role\": \"assistant\", \"content\": format_results_page(current_state)})\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "                user_input_update = gr.update(placeholder=\"Escribe 'siguiente' o elige una opción\")\n",
    "            \n",
    "            else:\n",
    "                history.append({\"role\": \"assistant\", \"content\": \"Por favor, utiliza uno de los botones para continuar.\"})\n",
    "                (btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update) = make_buttons_interactive(True)\n",
    "\n",
    "            return history, current_state, button_row_update, btn_author_update, btn_topic_update, btn_combo_update, btn_bye_update, user_input_update\n",
    "\n",
    "        def handle_choice(history, current_state, choice):\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Has elegido: **{choice}**\"})\n",
    "            prompt_message, mode = \"\", current_state[\"current_mode\"]\n",
    "            if choice == \"Búsqueda por autoría\":\n",
    "                mode = \"waiting_for_author\"\n",
    "                prompt_message = \"Por favor, escribe el nombre del autor/a o autores/as (separados por comas).\"\n",
    "            elif choice == \"Búsqueda por temática\":\n",
    "                mode = \"waiting_for_topic\"\n",
    "                prompt_message = \"Por favor, escribe la temática que te interesa.\"\n",
    "            elif choice == \"Búsqueda por autoría y temática\":\n",
    "                mode = \"waiting_for_combo_author\"\n",
    "                prompt_message = \"Perfecto. Primero, escribe el nombre del autor.\"\n",
    "            current_state[\"current_mode\"] = mode\n",
    "            history.append({\"role\": \"assistant\", \"content\": prompt_message})\n",
    "            b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            return history, current_state, b1, b2, b3, b4, gr.update(interactive=True, placeholder=\"Escribe aquí...\")\n",
    "\n",
    "        def handle_bye(history, current_state):\n",
    "            if current_state[\"user_name\"]:\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"Adiós {current_state['user_name']}, ¡recuerda que siempre estoy disponible para asistirte!\"})\n",
    "                current_state[\"current_mode\"] = \"ended\"\n",
    "            b1, b2, b3, b4 = make_buttons_interactive(False)\n",
    "            return history, current_state, b1, b2, b3, b4, gr.update(interactive=False, placeholder=\"Conversación finalizada.\")\n",
    "\n",
    "        user_input.submit(handle_user_input, [user_input, chatbot, chat_state], [chatbot, chat_state, button_row, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_author.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_topic.click(lambda h, s: handle_choice(h, s, \"Búsqueda por temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_combo.click(lambda h, s: handle_choice(h, s, \"Búsqueda por autoría y temática\"), [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "        btn_bye.click(handle_bye, [chatbot, chat_state], [chatbot, chat_state, btn_author, btn_topic, btn_combo, btn_bye, user_input])\n",
    "\n",
    "    print(\"\\nLanzando la interfaz de CEMIA con botones...\")\n",
    "    demo.launch(inline=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
